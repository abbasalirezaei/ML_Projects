{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3949e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 2072 to 2575\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           16512 non-null  float64\n",
      " 1   latitude            16512 non-null  float64\n",
      " 2   housing_median_age  16512 non-null  float64\n",
      " 3   total_rooms         16512 non-null  float64\n",
      " 4   total_bedrooms      16336 non-null  float64\n",
      " 5   population          16512 non-null  float64\n",
      " 6   households          16512 non-null  float64\n",
      " 7   median_income       16512 non-null  float64\n",
      " 8   median_house_value  16512 non-null  float64\n",
      " 9   ocean_proximity     16512 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom pandas.plotting import scatter_matrix\\nfrom sklearn.impute import SimpleImputer\\n\\n%load_ext nb_black\\n# =====================\\nhousing = pd.read_csv(r\\\"housing.csv\\\")\\n\\n# housing.head(20)\\n# housing.tail()\\n# housing[1:5]\\n# housing.shape\\n# housing.info()\\n# housing.columns\\n# housing['ocean_proximity'].unique()\\n# housing[\\\"ocean_proximity\\\"].value_counts()\\n# housing[housing[\\\"ocean_proximity\\\"] == \\\"ISLAND\\\"]\\n# housing[[\\\"population\\\", \\\"median_income\\\"]][housing[\\\"ocean_proximity\\\"] == \\\"ISLAND\\\"]\\n# housing.describe()\\n# housing.hist(bins=50, figsize=(20, 15))\\n# plt.show()\\n# ===================\\n\\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=2)\\ntrain_set.shape\\n\\ntest_set.shape\\ntrain_set.info()\\n# housing.info()\";\n",
       "                var nbb_formatted_code = \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom pandas.plotting import scatter_matrix\\nfrom sklearn.impute import SimpleImputer\\n\\n%load_ext nb_black\\n# =====================\\nhousing = pd.read_csv(r\\\"housing.csv\\\")\\n\\n# housing.head(20)\\n# housing.tail()\\n# housing[1:5]\\n# housing.shape\\n# housing.info()\\n# housing.columns\\n# housing['ocean_proximity'].unique()\\n# housing[\\\"ocean_proximity\\\"].value_counts()\\n# housing[housing[\\\"ocean_proximity\\\"] == \\\"ISLAND\\\"]\\n# housing[[\\\"population\\\", \\\"median_income\\\"]][housing[\\\"ocean_proximity\\\"] == \\\"ISLAND\\\"]\\n# housing.describe()\\n# housing.hist(bins=50, figsize=(20, 15))\\n# plt.show()\\n# ===================\\n\\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=2)\\ntrain_set.shape\\n\\ntest_set.shape\\ntrain_set.info()\\n# housing.info()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "%load_ext nb_black\n",
    "# =====================\n",
    "housing = pd.read_csv(r\"housing.csv\")\n",
    "\n",
    "# housing.head(20)\n",
    "# housing.tail()\n",
    "# housing[1:5]\n",
    "# housing.shape\n",
    "# housing.info()\n",
    "# housing.columns\n",
    "# housing['ocean_proximity'].unique()\n",
    "# housing[\"ocean_proximity\"].value_counts()\n",
    "# housing[housing[\"ocean_proximity\"] == \"ISLAND\"]\n",
    "# housing[[\"population\", \"median_income\"]][housing[\"ocean_proximity\"] == \"ISLAND\"]\n",
    "# housing.describe()\n",
    "# housing.hist(bins=50, figsize=(20, 15))\n",
    "# plt.show()\n",
    "# ===================\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=2)\n",
    "train_set.shape\n",
    "\n",
    "test_set.shape\n",
    "train_set.info()\n",
    "# housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e4a5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median_house_value    1.000000\n",
       "median_income         0.689659\n",
       "total_rooms           0.133218\n",
       "housing_median_age    0.108626\n",
       "households            0.063245\n",
       "total_bedrooms        0.047478\n",
       "population           -0.027441\n",
       "longitude            -0.046754\n",
       "latitude             -0.143970\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# becuse we have lable and we wanna predet the label sooo we have supervised algoritm\\n# and becouse of we donot want classification so we have regresion\\ntrain_set.tail()\\ndata = train_set.copy()\\n# we have the map of california\\n# each point in the plot bellow is a row of the our table\\n# alpha --> \\u0686\\u06af\\u0627\\u0644\\u06cc \\u0645\\u0646\\u0627\\u0637\\u0642 \\u0631\\u0648 \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647\\n# data.plot(kind=\\\"scatter\\\", x=\\\"longitude\\\", y=\\\"latitude\\\", figsize=(10, 7), alpha=0.2)\\n# ===============================\\n# s for \\u0634\\u0639\\u0627\\u0639 \\u0647\\u0631 \\u062f\\u0627\\u06cc\\u0631\\u0647 \\u062f\\u0631\\n# data.plot(kind=\\\"scatter\\\", x=\\\"longitude\\\", y=\\\"latitude\\\",\\n#           s=data['population']/30,label='population'\\n#           ,figsize=(10, 7), alpha=0.2)\\n# ============================\\n# \\u0645\\u0627 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0642\\u06cc\\u0645\\u062a \\u062e\\u0627\\u0646\\u0647 \\u062f\\u0631 \\u0647\\u0631\\u0645\\u0646\\u0637\\u0642\\u0647 \\u06a9\\u0647 \\u0648\\u06cc\\u0698\\u06af\\u06cc \\u0627\\u0635\\u0644\\u06cc \\u0645\\u0627 \\u0647\\u0633\\u062a \\u0648 \\u0647\\u062f\\u0641 \\u0645\\u0627 \\u067e\\u06cc\\u0634 \\u0628\\u06cc\\u06cc\\u0646 \\u06cc\\n# \\u0627\\u0648\\u0646 \\u0647\\u0633\\u062a \\u0631\\u0648 \\u0647\\u0645 \\u0648\\u0627\\u0631\\u062f \\u0628\\u0627\\u0632\\u06cc \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645\\n#\\n# data.plot(\\n#     kind=\\\"scatter\\\",\\n#     x=\\\"longitude\\\",\\n#     y=\\\"latitude\\\",\\n#     s=data[\\\"population\\\"] / 30,\\n#     label=\\\"population\\\",\\n#     c=\\\"median_house_value\\\",\\n#     cmap=plt.get_cmap(\\\"jet\\\"),\\n#     figsize=(10, 7),\\n#     alpha=0.2,\\n# )\\n\\n\\n# ================\\n# \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0646 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0647\\u0627\\n# \\u0627\\u0632 \\u0647\\u0645\\u0647 \\u0645\\u0647\\u0645 \\u062a\\u0631 \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0646 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc  \\u062f\\u06cc\\u06af\\u0631 \\u0628\\u0627 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u062f\\u0641 \\u0627\\u0633...\\n\\n\\n# =============\\n# standard correlation coeficient \\n# \\u0645\\u0642\\u062f\\u0627\\u0631\\u0634 \\u0627\\u0632 -\\u06f1 \\u062a\\u0627 \\u06f1 \\u0647\\u0633\\u062a ... \\u06a9\\u0647 \\u062f\\u0631\\u062c\\u0647 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0631\\u0648 \\u0645\\u06cc\\u06af\\u0647 \\u0647\\u0631 \\u0686\\u06cc \\u0628\\u0647  \\u06f1 \\u0646\\u0632\\u062f\\u06cc\\u06a9 \\u062a\\u0631 \\u0628\\u0634\\u0647 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc\\n# \\u0645\\u062b\\u0628\\u062a \\u062f\\u0627\\u0631\\u0647 ...\\n# \\u0648 \\u0647\\u0631\\u0686\\u06cc \\u0628\\u0647 -\\u06f1 \\u0646\\u0632\\u062f\\u06cc\\u06af \\u0628\\u0634\\u06af\\u06cc \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0632\\u06cc\\u0627\\u062f\\u06cc \\u062f\\u0627\\u0631\\u0647 \\u0627\\u0645\\u0627 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0645\\u0646\\u0641\\u06cc \\n# \\u0636\\u0631\\u06cc\\u0628 \\u0627\\u0633\\u062a\\u0627\\u0646\\u062f\\u0627\\u0631\\u062f \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc\\n# \\u0645\\u0634\\u06a9\\u0644\\u0634 \\u0627\\u06cc\\u0646\\u0647 \\u0641\\u0642\\u0637 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0647\\u0627\\u06cc \\u062e\\u0637\\u06cc \\u0631\\u0648 \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647\\n\\n# find relation of each posssible tuple\\ncorr_matrix = data.corr()\\n# print(corr_matrix)\\ncorr_matrix\\n\\n# becuse of we wanna relation of each row with median_house_value\\n\\n# corr_matrix[\\\"median_house_value\\\"]\\n\\ncorr_matrix[\\\"median_house_value\\\"].sort_values(ascending=False)\";\n",
       "                var nbb_formatted_code = \"# becuse we have lable and we wanna predet the label sooo we have supervised algoritm\\n# and becouse of we donot want classification so we have regresion\\ntrain_set.tail()\\ndata = train_set.copy()\\n# we have the map of california\\n# each point in the plot bellow is a row of the our table\\n# alpha --> \\u0686\\u06af\\u0627\\u0644\\u06cc \\u0645\\u0646\\u0627\\u0637\\u0642 \\u0631\\u0648 \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647\\n# data.plot(kind=\\\"scatter\\\", x=\\\"longitude\\\", y=\\\"latitude\\\", figsize=(10, 7), alpha=0.2)\\n# ===============================\\n# s for \\u0634\\u0639\\u0627\\u0639 \\u0647\\u0631 \\u062f\\u0627\\u06cc\\u0631\\u0647 \\u062f\\u0631\\n# data.plot(kind=\\\"scatter\\\", x=\\\"longitude\\\", y=\\\"latitude\\\",\\n#           s=data['population']/30,label='population'\\n#           ,figsize=(10, 7), alpha=0.2)\\n# ============================\\n# \\u0645\\u0627 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0642\\u06cc\\u0645\\u062a \\u062e\\u0627\\u0646\\u0647 \\u062f\\u0631 \\u0647\\u0631\\u0645\\u0646\\u0637\\u0642\\u0647 \\u06a9\\u0647 \\u0648\\u06cc\\u0698\\u06af\\u06cc \\u0627\\u0635\\u0644\\u06cc \\u0645\\u0627 \\u0647\\u0633\\u062a \\u0648 \\u0647\\u062f\\u0641 \\u0645\\u0627 \\u067e\\u06cc\\u0634 \\u0628\\u06cc\\u06cc\\u0646 \\u06cc\\n# \\u0627\\u0648\\u0646 \\u0647\\u0633\\u062a \\u0631\\u0648 \\u0647\\u0645 \\u0648\\u0627\\u0631\\u062f \\u0628\\u0627\\u0632\\u06cc \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645\\n#\\n# data.plot(\\n#     kind=\\\"scatter\\\",\\n#     x=\\\"longitude\\\",\\n#     y=\\\"latitude\\\",\\n#     s=data[\\\"population\\\"] / 30,\\n#     label=\\\"population\\\",\\n#     c=\\\"median_house_value\\\",\\n#     cmap=plt.get_cmap(\\\"jet\\\"),\\n#     figsize=(10, 7),\\n#     alpha=0.2,\\n# )\\n\\n\\n# ================\\n# \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0646 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0647\\u0627\\n# \\u0627\\u0632 \\u0647\\u0645\\u0647 \\u0645\\u0647\\u0645 \\u062a\\u0631 \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0646 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc  \\u062f\\u06cc\\u06af\\u0631 \\u0628\\u0627 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u062f\\u0641 \\u0627\\u0633...\\n\\n\\n# =============\\n# standard correlation coeficient\\n# \\u0645\\u0642\\u062f\\u0627\\u0631\\u0634 \\u0627\\u0632 -\\u06f1 \\u062a\\u0627 \\u06f1 \\u0647\\u0633\\u062a ... \\u06a9\\u0647 \\u062f\\u0631\\u062c\\u0647 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0631\\u0648 \\u0645\\u06cc\\u06af\\u0647 \\u0647\\u0631 \\u0686\\u06cc \\u0628\\u0647  \\u06f1 \\u0646\\u0632\\u062f\\u06cc\\u06a9 \\u062a\\u0631 \\u0628\\u0634\\u0647 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc\\n# \\u0645\\u062b\\u0628\\u062a \\u062f\\u0627\\u0631\\u0647 ...\\n# \\u0648 \\u0647\\u0631\\u0686\\u06cc \\u0628\\u0647 -\\u06f1 \\u0646\\u0632\\u062f\\u06cc\\u06af \\u0628\\u0634\\u06af\\u06cc \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0632\\u06cc\\u0627\\u062f\\u06cc \\u062f\\u0627\\u0631\\u0647 \\u0627\\u0645\\u0627 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0645\\u0646\\u0641\\u06cc\\n# \\u0636\\u0631\\u06cc\\u0628 \\u0627\\u0633\\u062a\\u0627\\u0646\\u062f\\u0627\\u0631\\u062f \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc\\n# \\u0645\\u0634\\u06a9\\u0644\\u0634 \\u0627\\u06cc\\u0646\\u0647 \\u0641\\u0642\\u0637 \\u0648\\u0627\\u0628\\u0633\\u062a\\u06af\\u06cc \\u0647\\u0627\\u06cc \\u062e\\u0637\\u06cc \\u0631\\u0648 \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647\\n\\n# find relation of each posssible tuple\\ncorr_matrix = data.corr()\\n# print(corr_matrix)\\ncorr_matrix\\n\\n# becuse of we wanna relation of each row with median_house_value\\n\\n# corr_matrix[\\\"median_house_value\\\"]\\n\\ncorr_matrix[\\\"median_house_value\\\"].sort_values(ascending=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# becuse we have lable and we wanna predet the label sooo we have supervised algoritm\n",
    "# and becouse of we donot want classification so we have regresion\n",
    "train_set.tail()\n",
    "data = train_set.copy()\n",
    "# we have the map of california\n",
    "# each point in the plot bellow is a row of the our table\n",
    "# alpha --> چگالی مناطق رو به ما میده\n",
    "# data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10, 7), alpha=0.2)\n",
    "# ===============================\n",
    "# s for شعاع هر دایره در\n",
    "# data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",\n",
    "#           s=data['population']/30,label='population'\n",
    "#           ,figsize=(10, 7), alpha=0.2)\n",
    "# ============================\n",
    "# ما میانگین قیمت خانه در هرمنطقه که ویژگی اصلی ما هست و هدف ما پیش بیین ی\n",
    "# اون هست رو هم وارد بازی می کنیم\n",
    "#\n",
    "# data.plot(\n",
    "#     kind=\"scatter\",\n",
    "#     x=\"longitude\",\n",
    "#     y=\"latitude\",\n",
    "#     s=data[\"population\"] / 30,\n",
    "#     label=\"population\",\n",
    "#     c=\"median_house_value\",\n",
    "#     cmap=plt.get_cmap(\"jet\"),\n",
    "#     figsize=(10, 7),\n",
    "#     alpha=0.2,\n",
    "# )\n",
    "\n",
    "\n",
    "# ================\n",
    "# پیدا کردن وابستگی ها\n",
    "# از همه مهم تر پیدا کردن وابستگی ستون های  دیگر با ستون هدف اس...\n",
    "\n",
    "\n",
    "# =============\n",
    "# standard correlation coeficient \n",
    "# مقدارش از -۱ تا ۱ هست ... که درجه وابستگی رو میگه هر چی به  ۱ نزدیک تر بشه وابستگی\n",
    "# مثبت داره ...\n",
    "# و هرچی به -۱ نزدیگ بشگی وابستگی زیادی داره اما وابستگی منفی \n",
    "# ضریب استاندارد وابستگی\n",
    "# مشکلش اینه فقط وابستگی های خطی رو به ما میده\n",
    "\n",
    "# find relation of each posssible tuple\n",
    "corr_matrix = data.corr()\n",
    "# print(corr_matrix)\n",
    "corr_matrix\n",
    "\n",
    "# becuse of we wanna relation of each row with median_house_value\n",
    "\n",
    "# corr_matrix[\"median_house_value\"]\n",
    "\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319e5b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"features = [\\\"median_house_value\\\", \\\"median_income\\\", \\\"total_rooms\\\", \\\"housing_median_age\\\"]\\n# scatter_matrix(data[features], figsize=(15, 10))\\n# plt.show()\";\n",
       "                var nbb_formatted_code = \"features = [\\\"median_house_value\\\", \\\"median_income\\\", \\\"total_rooms\\\", \\\"housing_median_age\\\"]\\n# scatter_matrix(data[features], figsize=(15, 10))\\n# plt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "# scatter_matrix(data[features], figsize=(15, 10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10829b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066f82da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>-119.84</td>\n",
       "      <td>36.77</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>1.4817</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>-117.80</td>\n",
       "      <td>33.68</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>6.9133</td>\n",
       "      <td>274100.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>-120.19</td>\n",
       "      <td>36.60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>1.5536</td>\n",
       "      <td>58300.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>-118.32</td>\n",
       "      <td>34.10</td>\n",
       "      <td>31.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1.5284</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16541</th>\n",
       "      <td>-121.23</td>\n",
       "      <td>37.79</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>4.0815</td>\n",
       "      <td>117900.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "2072     -119.84     36.77                 6.0       1853.0           473.0   \n",
       "10600    -117.80     33.68                 8.0       2032.0           349.0   \n",
       "2494     -120.19     36.60                25.0        875.0           214.0   \n",
       "4284     -118.32     34.10                31.0        622.0           229.0   \n",
       "16541    -121.23     37.79                21.0       1922.0           373.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "2072       1397.0       417.0         1.4817             72000.0   \n",
       "10600       862.0       340.0         6.9133            274100.0   \n",
       "2494        931.0       214.0         1.5536             58300.0   \n",
       "4284        597.0       227.0         1.5284            200000.0   \n",
       "16541      1130.0       372.0         4.0815            117900.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "2072           INLAND  \n",
       "10600       <1H OCEAN  \n",
       "2494           INLAND  \n",
       "4284        <1H OCEAN  \n",
       "16541          INLAND  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"data.head()\";\n",
       "                var nbb_formatted_code = \"data.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746cc7e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>-119.84</td>\n",
       "      <td>36.77</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>1.4817</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>-117.80</td>\n",
       "      <td>33.68</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>6.9133</td>\n",
       "      <td>274100.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>-120.19</td>\n",
       "      <td>36.60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>1.5536</td>\n",
       "      <td>58300.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>-118.32</td>\n",
       "      <td>34.10</td>\n",
       "      <td>31.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1.5284</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16541</th>\n",
       "      <td>-121.23</td>\n",
       "      <td>37.79</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>4.0815</td>\n",
       "      <td>117900.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "2072     -119.84     36.77                 6.0       1853.0           473.0   \n",
       "10600    -117.80     33.68                 8.0       2032.0           349.0   \n",
       "2494     -120.19     36.60                25.0        875.0           214.0   \n",
       "4284     -118.32     34.10                31.0        622.0           229.0   \n",
       "16541    -121.23     37.79                21.0       1922.0           373.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "2072       1397.0       417.0         1.4817             72000.0   \n",
       "10600       862.0       340.0         6.9133            274100.0   \n",
       "2494        931.0       214.0         1.5536             58300.0   \n",
       "4284        597.0       227.0         1.5284            200000.0   \n",
       "16541      1130.0       372.0         4.0815            117900.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "2072           INLAND  \n",
       "10600       <1H OCEAN  \n",
       "2494           INLAND  \n",
       "4284        <1H OCEAN  \n",
       "16541          INLAND  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# data.plot(\\n#     kind=\\\"scatter\\\",\\n#     x=\\\"median_income\\\",\\n#     y=\\\"median_house_value\\\",\\n#     figsize=(10, 7),\\n#     alpha=0.4,\\n# )\\n\\n# data[\\\"total_rooms_per_households\\\"] = data[\\\"total_rooms\\\"] / data[\\\"households\\\"]\\n#\\n# data[\\\"total_bedrooms_per_total_rooms\\\"] = data[\\\"total_bedrooms\\\"] / data[\\\"total_rooms\\\"]\\n\\n# data[\\\"population_per_households\\\"] = data[\\\"population\\\"] / data[\\\"households\\\"]\\n\\ncorr_matrix = data.corr()\\n# print(corr_matrix)\\ncorr_matrix\\n\\n# corr_matrix[\\\"median_house_value\\\"]\\n\\ncorr_matrix[\\\"median_house_value\\\"].sort_values(ascending=False)\\ndata.head()\";\n",
       "                var nbb_formatted_code = \"# data.plot(\\n#     kind=\\\"scatter\\\",\\n#     x=\\\"median_income\\\",\\n#     y=\\\"median_house_value\\\",\\n#     figsize=(10, 7),\\n#     alpha=0.4,\\n# )\\n\\n# data[\\\"total_rooms_per_households\\\"] = data[\\\"total_rooms\\\"] / data[\\\"households\\\"]\\n#\\n# data[\\\"total_bedrooms_per_total_rooms\\\"] = data[\\\"total_bedrooms\\\"] / data[\\\"total_rooms\\\"]\\n\\n# data[\\\"population_per_households\\\"] = data[\\\"population\\\"] / data[\\\"households\\\"]\\n\\ncorr_matrix = data.corr()\\n# print(corr_matrix)\\ncorr_matrix\\n\\n# corr_matrix[\\\"median_house_value\\\"]\\n\\ncorr_matrix[\\\"median_house_value\\\"].sort_values(ascending=False)\\ndata.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data.plot(\n",
    "#     kind=\"scatter\",\n",
    "#     x=\"median_income\",\n",
    "#     y=\"median_house_value\",\n",
    "#     figsize=(10, 7),\n",
    "#     alpha=0.4,\n",
    "# )\n",
    "\n",
    "# data[\"total_rooms_per_households\"] = data[\"total_rooms\"] / data[\"households\"]\n",
    "#\n",
    "# data[\"total_bedrooms_per_total_rooms\"] = data[\"total_bedrooms\"] / data[\"total_rooms\"]\n",
    "\n",
    "# data[\"population_per_households\"] = data[\"population\"] / data[\"households\"]\n",
    "\n",
    "corr_matrix = data.corr()\n",
    "# print(corr_matrix)\n",
    "corr_matrix\n",
    "\n",
    "# corr_matrix[\"median_house_value\"]\n",
    "\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a095ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abc/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>population_per_household</th>\n",
       "      <th>bedrooms_per_rooms</th>\n",
       "      <th>prox_&lt;1H OCEAN</th>\n",
       "      <th>prox_INLAND</th>\n",
       "      <th>prox_ISLAND</th>\n",
       "      <th>prox_NEAR BAY</th>\n",
       "      <th>prox_NEAR OCEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.137635</td>\n",
       "      <td>0.534564</td>\n",
       "      <td>-1.795939</td>\n",
       "      <td>-0.357368</td>\n",
       "      <td>-0.154134</td>\n",
       "      <td>-0.032827</td>\n",
       "      <td>-0.218173</td>\n",
       "      <td>-1.258403</td>\n",
       "      <td>-1.167387</td>\n",
       "      <td>-0.425185</td>\n",
       "      <td>0.024660</td>\n",
       "      <td>0.658887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.879836</td>\n",
       "      <td>-0.909979</td>\n",
       "      <td>-1.637178</td>\n",
       "      <td>-0.276515</td>\n",
       "      <td>-0.447238</td>\n",
       "      <td>-0.494784</td>\n",
       "      <td>-0.417841</td>\n",
       "      <td>1.610623</td>\n",
       "      <td>0.586611</td>\n",
       "      <td>0.247344</td>\n",
       "      <td>-0.049867</td>\n",
       "      <td>-0.669911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.312201</td>\n",
       "      <td>0.455091</td>\n",
       "      <td>-0.287715</td>\n",
       "      <td>-0.799127</td>\n",
       "      <td>-0.766343</td>\n",
       "      <td>-0.435204</td>\n",
       "      <td>-0.744572</td>\n",
       "      <td>-1.220425</td>\n",
       "      <td>-1.286288</td>\n",
       "      <td>-0.580880</td>\n",
       "      <td>0.116155</td>\n",
       "      <td>0.488784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.620480</td>\n",
       "      <td>-0.713633</td>\n",
       "      <td>0.188566</td>\n",
       "      <td>-0.913406</td>\n",
       "      <td>-0.730887</td>\n",
       "      <td>-0.723603</td>\n",
       "      <td>-0.710862</td>\n",
       "      <td>-1.233736</td>\n",
       "      <td>-0.056493</td>\n",
       "      <td>-1.172623</td>\n",
       "      <td>-0.041209</td>\n",
       "      <td>2.455426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.830911</td>\n",
       "      <td>1.011403</td>\n",
       "      <td>-0.605236</td>\n",
       "      <td>-0.326201</td>\n",
       "      <td>-0.390508</td>\n",
       "      <td>-0.263373</td>\n",
       "      <td>-0.334862</td>\n",
       "      <td>0.114837</td>\n",
       "      <td>-0.769028</td>\n",
       "      <td>-0.107958</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>-0.314810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0  -0.137635  0.534564           -1.795939    -0.357368       -0.154134   \n",
       "1   0.879836 -0.909979           -1.637178    -0.276515       -0.447238   \n",
       "2  -0.312201  0.455091           -0.287715    -0.799127       -0.766343   \n",
       "3   0.620480 -0.713633            0.188566    -0.913406       -0.730887   \n",
       "4  -0.830911  1.011403           -0.605236    -0.326201       -0.390508   \n",
       "\n",
       "   population  households  median_income  median_house_value  \\\n",
       "0   -0.032827   -0.218173      -1.258403           -1.167387   \n",
       "1   -0.494784   -0.417841       1.610623            0.586611   \n",
       "2   -0.435204   -0.744572      -1.220425           -1.286288   \n",
       "3   -0.723603   -0.710862      -1.233736           -0.056493   \n",
       "4   -0.263373   -0.334862       0.114837           -0.769028   \n",
       "\n",
       "   rooms_per_household  population_per_household  bedrooms_per_rooms  \\\n",
       "0            -0.425185                  0.024660            0.658887   \n",
       "1             0.247344                 -0.049867           -0.669911   \n",
       "2            -0.580880                  0.116155            0.488784   \n",
       "3            -1.172623                 -0.041209            2.455426   \n",
       "4            -0.107958                 -0.003921           -0.314810   \n",
       "\n",
       "   prox_<1H OCEAN  prox_INLAND  prox_ISLAND  prox_NEAR BAY  prox_NEAR OCEAN  \n",
       "0             0.0          1.0          0.0            0.0              0.0  \n",
       "1             1.0          0.0          0.0            0.0              0.0  \n",
       "2             0.0          1.0          0.0            0.0              0.0  \n",
       "3             1.0          0.0          0.0            0.0              0.0  \n",
       "4             0.0          1.0          0.0            0.0              0.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"from sklearn.base import BaseEstimator, TransformerMixin\\n\\n# ==prapare the data\\n\\n# numerical data            ===>         missing values\\n# categorical and text data ===>labelencoder ,onehotencoder\\n# numerical data            ===> feature scaling\\n# numerical data            ===> custom transformers\\n\\n# first make a anoter copy of train set\\ndf = train_set.copy()\\n# df.head()\\n# second you have find that witch data you dont wanna change and\\n# whtch data you wanna change\\n# \\u0686\\u0648\\u0646 \\u0645\\u0627 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0647\\u06cc\\u0645 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0642\\u06cc\\u0645\\u06cc\\u062a \\u062e\\u0648\\u0646\\u0647 \\u062f\\u0631 \\u06cc\\u06a9 \\u0645\\u0646\\u0637\\u0642\\u0647 \\u0631\\u0648 \\u067e\\u06cc\\u0634 \\u0628\\u06cc\\u0646\\u06cc \\u06a9\\u0646\\u06cc\\u0645 \\u0646\\u0628\\u0627\\u06cc\\u062f \\u0627\\u0648\\u0646 \\u062a\\u063a\\u06cc\\u06cc\\u0631 \\u06a9\\u0646\\u0647\\n# \\u067e\\u0633 \\u0645\\u06cc\\u0627\\u06cc\\u06cc\\u0645 \\u06cc\\u06a9 \\u06a9\\u067e\\u06cc \\u0627\\u0632 \\u0627\\u0648\\u0646 \\u0631\\u0648 \\u062a\\u0647\\u06cc\\u0647 \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645\\n# \\u0648\\u0627\\u06cc\\u0646\\u0648 \\u062c\\u062f\\u0627 \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645 \\u0627\\u0632 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u062f\\u06cc\\u06af\\u0647 \\u0648 \\u0628\\u0631\\u0627\\u06cc \\u067e\\u0627\\u06a9 \\u0633\\u0627\\u0632\\u06cc \\u0628\\u0647 \\u0627\\u06cc\\u0646 \\u0646\\u0628\\u0627\\u06cc\\u062f \\u062f\\u0633\\u062a \\u0628\\u0632\\u0646\\u06cc\\u0645...\\ndf_label = df[\\\"median_house_value\\\"].copy()\\ndf.drop(\\\"median_house_value\\\", axis=1)\\n# df.info()\\n\\n# \\u062e\\u0628 \\u0645\\u0627 \\u062f\\u0631 \\u0627\\u062a\\u0627\\u0642 \\u0647\\u0627\\u06cc \\u062e\\u0648\\u0627\\u0628 \\u0647\\u0627 \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0646\\u0627\\u0644 \\u062f\\u0627\\u0631\\u06cc\\u0645\\n# \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u0627\\u0648\\u0646 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645\\n# \\u0633\\u067e\\u0633 \\u0628\\u0627\\u06cc\\u062f \\u0645\\u0642\\u0627\\u062f\\u06cc\\u0631 \\u0645\\u062a\\u0646\\u06cc \\u0631\\u0648\\u0628\\u0627\\u06cc\\u062f \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645 \\u0686\\u0648\\u0646 \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0647\\u0627 \\u0627\\u06cc\\u0646\\u0627 \\u0631\\u0648 \\u0646\\u0645\\u06cc \\u0641\\u0647\\u0645\\u0646\\n\\n# df_num = df.drop(\\\"ocean_proximity\\\", axis=1)\\n# df_num.head()\\n# df_numd is all data that all of them are numercal ...\\n\\n# \\u062d\\u0627\\u0644 \\u0627\\u0632 \\u0628\\u06cc\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc \\u0645\\u0627 \\u0641\\u0642\\u0637 \\u062a\\u0639\\u0627\\u062f \\u0627\\u062a\\u0627\\u0642 \\u0647\\u0627 \\u0645\\u0634\\u06a9\\u0644 \\u062f\\u0627\\u0631\\u0647\\n# \\u062d\\u0627\\u0644 \\u0633\\u0648\\u0627\\u0644\\u06cc \\u06a9\\u0647 \\u067e\\u06cc\\u0633\\u0634 \\u0645\\u06cc\\u0627\\u062f \\u0627\\u06cc\\u0646\\u0647 \\u06a9\\u0647 \\u0627\\u0632 \\u06a9\\u062c\\u0627\\u0645\\u06cc\\u062f\\u0648\\u0646\\u06cc\\u0645 \\u0648\\u0642\\u062a\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u062c\\u062f\\u06cc\\u062f \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647 \\u0647\\u0645\\u06cc\\u0646\\n#\\n# \\u0633\\u062a\\u0648\\u0646 \\u0645\\u0634\\u06a9\\u0644 \\u062f\\u0627\\u0631\\u0647 \\u0648 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc \\u062f\\u06cc\\u06af\\u0647 \\u0645\\u0634\\u06a9\\u0644 \\u0646\\u062f\\u0627\\u0631\\u0647\\u061f\\n# \\u067e\\u0633 \\u0645\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u06cc\\u06a9 \\u0631\\u0627\\u0647\\u06a9\\u0627\\u0631 \\u06a9\\u0644\\u06cc \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645\\n\\n\\n# \\u0645\\u0627 \\u0633\\u0647 \\u062a\\u0627 \\u0627\\u067e\\u0634\\u0646 \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u062e\\u0631\\u0627\\u0628 \\u062f\\u0627\\u0631\\u06cc\\u0645\\n# \\u0627\\u067e\\u0634\\u0646 \\u06cc\\u06a9\\n# \\u0627\\u0648\\u0646 \\u0633\\u0637\\u0631 \\u0647\\u0627 \\u0631\\u0648 \\u0627\\u0632 \\u0628\\u06cc\\u0646 \\u0628\\u0628\\u0631\\u06cc\\u0645 \\u0633\\u0637\\u0631 \\u0647\\u0627\\u06cc \\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0646\\u0627\\u0644 \\u0631\\u0648 \\u0627\\u0632 \\u0628\\u06cc\\u0646  \\u0628\\u0628\\u0631\\u06cc\\u0645\\n\\n\\n# \\u0627\\u067e\\u0634\\u0646 \\u062f\\u0648\\u0645\\n\\n# \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646\\u06cc \\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0646\\u0627\\u0644 \\u062f\\u0627\\u0631\\u0647 \\u0631\\u0648 \\u0627\\u0632 \\u0628\\u06cc\\u0646 \\u0628\\u0628\\u0631\\u06cc\\u0645\\n# \\u0645\\u0645\\u06a9\\u0646\\u0647 \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0628\\u062f\\u0631\\u062f \\u0628\\u062e\\u0648\\u0631\\u0647 ...\\n\\n\\n# \\u0627\\u067e\\u0634\\u0646 \\u0633\\u0648\\u0645\\n# \\u0627\\u067e\\u0634\\u0646 \\u0633\\u0648\\u0645 \\u0627\\u06cc\\u0646\\u0647 \\u06a9\\u0647 \\u0628\\u06cc\\u0627\\u06cc\\u0645 \\u0645\\u0642\\u062f\\u0627\\u0631 \\u062c\\u0627\\u06cc\\u06af\\u0632\\u06cc\\u0646 \\u0628\\u062f\\u06cc\\u0645\\n\\n# ==================== simpleImputer\\n# \\u0622\\u067e\\u0634\\u0646 \\u06f1\\n# df_num=df_num.dropna(subset=['total_bedrooms']) #option 1\\n\\n\\n# df_num.drop('total_bedrooms',axis=1)     #option 2\\n\\n# optin 3\\n# use th max or min or meadin or a const number for total_bedrooms\\n\\n# median = df_num[\\\"total_bedrooms\\\"].median()\\n# df_num[\\\"total_bedrooms\\\"].fillna(median)\\n# df_num.info()\\n\\n# \\u062e\\u0628 \\u0645\\u0627 \\u0641\\u0647\\u0645\\u06cc\\u062f\\u06cc\\u0645 \\u0627\\u067e\\u0634\\u0646 \\u0633\\u0647 \\u0628\\u0647\\u062a\\u0631\\u0647\\n# \\u062d\\u0627\\u0644\\u0627 \\u0628\\u0631\\u0627\\u06cc \\u0628\\u06a9\\u0627\\u0631 \\u06af\\u06cc\\u0631\\u06cc \\u0627\\u0632 \\u0627\\u06cc\\u0646 \\u0631\\u0648\\u0634 \\u0627\\u0632 \\u062a\\u0627\\u0628\\u0639 \\u0632\\u06cc\\u0631 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645\\n\\n# \\u0648\\u0642\\u062a\\u06cc \\u0627\\u0632 \\u0627\\u06cc\\u0646 \\u062a\\u0627\\u0628\\u0639 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645 \\u062e\\u06cc\\u0627\\u0644\\u0645\\u0648\\u0646 \\u0627\\u0631\\u062d\\u062a\\u0647 \\u0647\\u0631 \\u0633\\u062a\\u0648\\u0646\\u06cc \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0646\\u0627\\u0644 \\u062f\\u0627\\u0634\\u062a \\u0628\\u0631\\u0627\\u0645\\u0648\\u0646\\u062f\\u0631\\u0633\\u062a\\u0634 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\n# medain \\u0645\\u06cc\\u0627\\u0646\\u0647\\n# mean \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646\\n# \\u0645\\u0645\\u06a9\\u0646\\u0647 \\u0645\\u0627 \\u06cc\\u0647 \\u0639\\u062f\\u062f \\u067e\\u0631\\u062a \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645 \\u062f\\u0627\\u062e\\u0644 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0645\\u0648\\u0646 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0645\\u06cc\\u0627\\u062f \\u0627\\u0648\\u0646 \\u0631\\u0648 \\u0647\\u0645 \\u062f\\u0631\\u0646\\u0638\\u0631\\n# \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647 \\u0627\\u0645\\u0627 \\u0645\\u06cc\\u0627\\u0646\\u0647 \\u0632\\u06cc\\u0627\\u062f \\u0646\\u0647 \\u0686\\u0648\\u0646\\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u0633\\u0631\\u062a \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n# \\u0628\\u0639\\u062f \\u062f\\u0627\\u062f\\u0647\\u0648\\u06cc\\u0637 \\u0631\\u0648 \\u0628\\u0631 \\u0645\\u06cc\\u062f\\u0627\\u0631\\u0647\\n\\n# \\u0645\\u0642\\u0627\\u062f\\u0631\\u06cc \\u06a9\\u0647 \\u0628\\u06cc\\u0634\\u062a\\u0631\\u06cc\\u0646 \\u062a\\u06a9\\u0631\\u0627\\u0631 \\u062f\\u0634\\u0627\\u062a\\u0647\\n# most frequence\\n# \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0645\\u062a\\u0646\\u06cc \\u0647\\u0645 \\u0628\\u06a9\\u0627\\u0631 \\u0645\\u06cc\\u0631\\u0647 ...\\n\\nimputer = SimpleImputer(missing_values=np.nan, strategy=\\\"median\\\")\\nimputer.fit(df_num)\\n# \\u062a\\u0627\\u0628\\u0639 \\u0641\\u06cc\\u062a \\u0647\\u0645\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647 \\u0648 \\u0627\\u0632 \\u0627\\u0648\\u0646 \\u06cc\\u0627\\u062f \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647\\nX = imputer.transform(df_num)\\n# \\u062a\\u0631\\u0646\\u0633\\u0641\\u0648\\u0631\\u0645 \\u0686\\u06cc\\u0632\\u06cc \\u06a9\\u0647 \\u062f\\u0631 \\u0641\\u06cc\\u062a \\u0628\\u062f\\u0633\\u062a \\u0627\\u0648\\u0631\\u062f\\u0647 \\u0631\\u0648 \\u062c\\u0627\\u06cc\\u06af\\u0632\\u06cc\\u0646 \\u0645\\u06cc \\u06a9\\u0646\\u0647\\n# \\u062d\\u0627\\u0635\\u0644 \\u062a\\u0627\\u0628\\u0639 \\u062a\\u0631\\u0646\\u0633\\u0641\\u0648\\u0631\\u0645 \\u06cc\\u06a9 \\u0627\\u0631\\u0627\\u06cc\\u0647 \\u0646\\u0627\\u0645 \\u067e\\u0627\\u06cc \\u0647\\u0633\\u062a\\n\\n# \\u0645\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u0628\\u0647 \\u062f\\u06cc\\u062a\\u0627 \\u0641\\u0631\\u0645 \\u062a\\u0628\\u062f\\u06cc\\u0644\\u0634 \\u06a9\\u0646\\u06cc\\u0645 ...\\n\\ndf_num_imput_er = pd.DataFrame(X, columns=df_num.columns)\\n# df_num_imput_er.info()\\n# df_num.info()\\n# df_num_imput_er.head()\\n\\n\\n# ========================================custom transformers\\n# \\u0645\\u0645\\u06a9\\u0646\\u0647 \\u0645\\u0627 \\u0686\\u0646\\u062f \\u06cc\\u0646  \\u0633\\u062a\\u0648\\u0646 \\u062c\\u062f\\u06cc\\u062f \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645 \\u06a9\\u0647 \\u0628\\u062f\\u0631\\u062f\\u0645\\u0648\\u0646 \\u0645\\u06cc\\u062e\\u0648\\u0631\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062a\\u0648\\u0646\\u06cc\\u0645\\n# \\u0627\\u06cc\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0631\\u0648 \\u0628\\u0647 \\u062f\\u0627\\u062f\\u0647\\u0647\\u0627 \\u0645\\u0648\\u0646 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0646\\u0647\\n\\n# \\u0645\\u062b\\u0644\\u0627 \\u0645\\u0627 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0647\\u06cc\\u0645 \\u06cc\\u06a9 \\u0633\\u0631\\u06cc \\u0633\\u062a\\u0648\\u0646 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0646\\u06cc\\u0645 \\u0627\\u0632 \\u0627\\u06cc\\u0646 \\u0631\\u0627\\u0647 \\u0645\\u06cc\\u0631\\u06cc\\u0645\\n# \\u0645\\u062b\\u0644\\u0627 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u06cc \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u06cc\\u06a9 \\u0633\\u062a\\u0648\\u0646 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u06a9\\u0646\\u06cc \\u0628\\u0631\\u06cc\\u0632\\u06cc \\u062a\\u0648\\u06cc \\u06cc\\u06a9 \\u0633\\u062a\\u0648\\u0646 \\u062f\\u06cc\\u06af\\u0647\\n\\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\\n\\n\\nclass CombineAtteributeAdder(BaseEstimator, TransformerMixin):\\n    #     \\u0686\\u0648\\u0646 \\u0645\\u0627 \\u06cc\\u0627\\u062f\\u06af\\u06cc\\u0631\\u06cc \\u0646\\u062f\\u0627\\u0631\\u06cc\\u0645 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062a\\u0627\\u0628\\u0639 \\u0641\\u06cc\\u062a \\u0646\\u06cc\\u0627\\u0632 \\u0646\\u06cc\\u0633\\u062a \\u06a9\\u0627\\u0631 \\u06a9\\u0646\\u0647 ...\\n    # \\u0627\\u0645\\u0627 \\u0645\\u062b\\u0644\\u0627 \\u0627\\u06af\\u0631 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0633\\u062a\\u06cc \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u06a9\\u0646\\u0647 \\u0628\\u0627\\u06cc\\u062f \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u06cc\\u06a9 \\u06a9\\u0627\\u0631\\u06cc \\u0627\\u0646\\u062c\\u0627\\u0645 \\u0645\\u06cc\\u062f\\u0627\\u062f\\u06cc\\n    #    \\u06cc\\u0639\\u0646\\u06cc \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0645\\u06cc\\u0646\\u06af\\u06cc\\u0646 \\u0631\\u0627 \\u0627\\u0628\\u062a\\u062f\\u0627 \\u062f\\u0631 \\u0641\\u06cc\\u062a \\u062d\\u0633\\u0627\\u0628 \\u0645\\u06cc\\u06a9\\u0646\\u06cc \\u0648 \\u0627\\u0632 \\u0627\\u0648\\u0646 \\u062f\\u0631 \\u062a\\u0631\\u0646\\u0633\\u0641\\u0648\\u0631\\u0645 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645\\n\\n    def fit(self, X, y=None):\\n        return self\\n\\n    def transform(self, X, y=None):\\n        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\\n        population_per_household = X[:, population_ix] / X[:, household_ix]\\n\\n        bedrooms_per_rooms = X[:, bedrooms_ix] / X[:, rooms_ix]\\n\\n        return np.c_[\\n            X, rooms_per_household, population_per_household, bedrooms_per_rooms\\n        ]\\n\\n\\ncustom = CombineAtteributeAdder()\\n# \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u0627\\u0632 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0627\\u06cc\\u067e\\u06cc\\u0648\\u0648\\u062a\\u0631 \\u0634\\u062f\\u0647 \\u06a9\\u0627 \\u0631 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645 \\u0686\\u0648\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0646\\u0627\\u0644 \\u0646\\u062f\\u0627\\u0631\\u06cc\\u0645\\ndata_sustom_tr_tmp = custom.transform(df_num_imput_er.values)\\n\\ndata_sustom_tr = pd.DataFrame(data_sustom_tr_tmp)\\n# \\u06cc\\u06a9 \\u0644\\u06cc\\u0633\\u062a \\u0645\\u06cc\\u0633\\u0627\\u0632\\u06cc\\u0645 \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0645\\u06cc\\u06af\\u06cc\\u0645 \\u0647\\u0645\\u0647 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc \\u0642\\u0628\\u0644\\u06cc \\u0631\\u0648 \\u062f\\u0627\\u0631\\u06cc\\u0645 \\u0628\\u0639\\u062f \\u0633\\u0647 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0645\\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u06a9\\u0647 \\u0628\\u0627 \\u0627\\u067e\\u0646\\u062f \\u0628\\u0647\\u0634 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645\\n\\ncolumns = list(df_num_imput_er.columns)\\ncolumns.append(\\\"rooms_per_household\\\")\\ncolumns.append(\\\"population_per_household\\\")\\ncolumns.append(\\\"bedrooms_per_rooms\\\")\\n\\ndata_sustom_tr.columns = columns\\ndata_sustom_tr.head(10)\\n# data_sustom_tr.shape\\n# data.shape\\n\\n# \\u067e\\u0633 \\u0645\\u0627 \\u062f\\u0631 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc  \\u062a\\u0627 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062f\\u0648\\u062a\\u0627 \\u06a9\\u0627\\u0631 \\u0627\\u0646\\u062c\\u0627\\u0645 \\u062f\\u0627\\u062f\\u06cc\\u0645\\n# \\u06cc\\u06a9\\u06cc \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0646\\u0627\\u0644 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u062f\\u0648\\u0645 \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc\\u06cc \\u06a9\\u0647 \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0647 \\u0628\\u0648\\u062f\\u06cc\\u0645 \\u0631\\u0648 \\u0647\\u0645 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n\\n# \\u06cc\\u06a9 \\u0645\\u06cc\\u0634\\u0647 missing data\\n# \\u062f\\u0648 \\u0645\\u06cc\\u0634\\u0647 custom transform\\n# \\u06cc\\u06a9 \\u06a9\\u0627\\u0631 \\u062f\\u06cc\\u06af\\u0647 \\u0647\\u0645 \\u062f\\u0627\\u0631\\u06cc\\u0645 \\u06a9\\u0647 \\u0645\\u06cc\\u0634\\u0647\\n# feature scaling\\n# =============================================feature scaling===================================\\n#  \\u0645\\u0627 \\u0627\\u06af\\u0631 \\u06cc\\u06a9 \\u0633\\u0631\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645\\u0648 \\u0627\\u06cc\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u062e\\u06cc\\u0644\\u06cc \\u0645\\u062a\\u0646\\u0648\\u0639 \\u0628\\u0627\\u0634\\u0646\\n# \\u06cc\\u0639\\u0646\\u06cc \\u0628\\u06cc\\u0646 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0648 \\u0645\\u0627\\u06a9\\u0633\\u06cc\\u0645\\u0645 \\u0648 \\u0645\\u0646\\u06cc\\u0645\\u0645 \\u0627\\u0646\\u0647\\u0627 \\u0641\\u0627\\u0635\\u0644\\u0647 \\u0632\\u06cc\\u0627\\u062f\\u06cc \\u0628\\u0627\\u0634\\u0647\\n# \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0627\\u0631 \\u0646\\u0645\\u06cc\\u06a9\\u0646\\u0647\\n# \\u0628\\u0631\\u0627\\u06cc \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u0627\\u06cc\\u0646 \\u0645\\u0634\\u06a9\\u0644 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645\\n\\n# \\u0627\\u0628\\u062a\\u062f\\u0627 \\u062a\\u0627\\u0628\\u0639\\n# describe()\\n# \\u0631\\u0648 \\u0635\\u062f\\u0627 \\u0645\\u06cc\\u0632\\u0646\\u06cc\\u0645.\\n\\ndata_sustom_tr.describe()\\n\\n\\n# \\u0648\\u0642\\u062a\\u06cc \\u0627\\u06cc\\u0646\\u0648 \\u0635\\u062f\\u0627 \\u0628\\u0632\\u0646\\u06cc \\u0645\\u06cc\\u0628\\u06cc\\u0646\\u06cc \\u06a9\\u0647\\u0645\\u062b\\u0644\\u0627 \\u062f\\u0631\\n# total_bedrooms\\n# \\u0645\\u0646\\u06cc\\u0645\\u0645\\u0634 \\u06f2 \\u0647\\u0633\\u062a \\u0648 \\u0645\\u0627\\u06a9\\u0633\\u06cc\\u0645\\u0645\\u0634\\n# \\u06f6\\u06f0\\u06f0\\u06f0\\n# \\u062e\\u06cc\\u0644\\u06cc\\u06cc\\u06cc \\u0641\\u0627\\u0635\\u0644\\u0647 \\u0627\\u0633\\u062a\\u062a.\\n# \\u0628\\u0627\\u06cc\\u062f \\u0627\\u06cc\\u0646 \\u0641\\u0627\\u0635\\u0644\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645\\n\\n\\n# \\u062f\\u0648 \\u0631\\u0648\\u0634 \\u0628\\u0631\\u0627\\u06cc \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u0627\\u06cc\\u0646\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645 \\u0648\\u062c\\u0648\\u062f \\u062f\\u0627\\u0631\\u0647\\n\\n# 1-standardization\\n\\n\\n# 2-noramlizatin [0,1]\\n\\n# \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u062f\\u0631 \\u0627\\u06cc\\u0646 \\u0628\\u0627\\u0632\\u0647 \\u0645\\u06cc\\u0627\\u0631\\u0634\\u0648\\u0646\\n# \\u0648 \\u062a\\u063a\\u06cc\\u06cc\\u0631 \\u0645\\u0642\\u06cc\\u0627\\u0633 \\u0645\\u06cc\\u062f\\u0647 ...\\n\\n# \\u0645\\u0646\\u0647\\u0627\\u06cc \\u0645\\u0646\\u06cc\\u0645\\u0645 \\u0645\\u06cc\\u06a9\\u0646\\u0647 \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0628\\u0639\\u062f\\u0634 \\u062a\\u0642\\u0633\\u06cc\\u0645 \\u0628\\u0631 \\u0645\\u0627\\u06a9\\u0633 \\u0645\\u0646\\u0647\\u0627\\u06cc \\u0645\\u06cc\\u0646 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\n# \\u06cc\\u06a9 \\u0627\\u06cc\\u0631\\u0627\\u062f \\u0628\\u0632\\u0631\\u06af \\u0648\\u062c\\u0648\\u062f \\u062f\\u0627\\u0631\\u0647 \\u0627\\u06cc\\u0631\\u0627\\u062f\\u0634 \\u0632\\u0645\\u0627\\u0646\\u06cc\\u0647 \\u06a9\\u0647 \\u06cc\\u06a9 \\u062f\\u0627\\u062f\\u0647 \\u067e\\u0631\\u062a \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645\\n# \\u0645\\u062b\\u0644\\u0627 \\u0627\\u06af\\u0631 \\u0645\\u0642\\u0627\\u062f\\u06cc\\u0631 \\u0645\\u0627 \\u0628\\u0647 \\u0635\\u0648\\u0631\\u062a \\u0632\\u06cc\\u0631 \\u0628\\u0627\\u0634\\u062f\\n# 1,3,4,5,7,100\\n\\n# \\u0627\\u06cc\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u06f1\\u06f0\\u06f0 \\u062f\\u0627\\u062f\\u0647 \\u067e\\u0631\\u062a\\u0647\\n\\n# \\u06a9\\u0644 \\u0627\\u0639\\u062f\\u0627\\u062f \\u0631\\u0648 \\u0628\\u06cc\\u0646 \\u0635\\u0641\\u0631 \\u062a\\u0627 \\u0647\\u0634\\u062a \\u062f\\u0647\\u0645 \\u0642\\u0631\\u0627\\u0631 \\u0645\\u06cc\\u062f\\u0647 \\u0648 \\u06f1\\u06f0\\u06f0 \\u0631\\u0648 \\u06cc\\u06a9 \\u0642\\u0631\\u0627\\u0631 \\u0645\\u06cc\\u062f\\u0647...\\n\\n\\n# \\u0645\\u0632\\u06cc\\u062a \\u0646\\u0631\\u0645\\u0627\\u0644\\u06cc\\u0632\\u0634\\u0646\\n#\\n# \\u062f\\u0631 \\u0634\\u0628\\u06a9\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u0635\\u0628\\u06cc \\u062e\\u0648\\u0628\\u0647 \\u0686\\u0648\\u0646 \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0647\\u0627\\u06cc \\u0634\\u0628\\u06a9\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u0635\\u0628\\u06cc \\u062f\\u0631 \\u0635\\u0641\\u0631 \\u0648 \\u06cc\\u06a9 \\u06a9\\u0627\\u0631 \\u0645\\u06cc\\u06a9\\u0646\\u0646\\n\\n\\n# 1-standardization\\n# \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u0645\\u06cc\\u0627\\u062f \\u0648\\u0627\\u0631\\u06cc\\u0627\\u0646\\u0633 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\n# \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0631\\u0627 \\u0645\\u0646\\u0647\\u0627\\u06cc \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0645\\u06cc\\u06a9\\u0646\\u0647 \\u0648 \\u0633\\u067e\\u0633 \\u062a\\u0642\\u0633\\u06cc\\u0645 \\u0628\\u0631 \\u0648\\u0627\\u0631\\u06cc\\u0627\\u0646\\u0633 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n#\\n# \\u06cc\\u06a9 \\u0628\\u0627\\u0632\\u0647 \\u062c\\u062f\\u06cc\\u062f \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647 \\u0648 \\u0641\\u0627\\u0635\\u0644\\u0647\\u0647\\u0627 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\n# \\u0645\\u0627 \\u0628\\u0627 \\u0627\\u06cc\\u0646 \\u062a\\u0627\\u0628\\u0639 \\u062a\\u0645\\u0627\\u0645 \\u0645\\u0642\\u0627\\u062f\\u06cc\\u0631 \\u062f\\u0627\\u062f\\u0647 \\u0627\\u06cc \\u0631\\u0648 \\u0645\\u06cc\\u0627\\u0631\\u06cc\\u0645 \\u0631\\u0648\\u06cc \\u06cc\\u06a9 \\u0628\\u0627\\u0632\\u0647 \\u062e\\u0627\\u0635 \\u06a9\\u0647 \\u0628\\u0631\\u0627\\u06cc \\u0627\\u06cc\\u0646 \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0645\\u0627 \\u0645\\u0646\\u0627\\u0633\\u0628\\u0647\\n\\nfeature_scal = StandardScaler()\\ndata_num_scaled_tr = pd.DataFrame(\\n    feature_scal.fit_transform(data_sustom_tr.values), columns=data_sustom_tr.columns\\n)\\n\\n\\ndata_num_scaled_tr.head()\\n\\n\\n#\\n# \\u062a\\u063a\\u06cc\\u06cc\\u0631\\u0627\\u062a\\u06cc \\u06a9\\u0647 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062f\\u0627\\u062f\\u06cc\\u0645 \\u0628\\u0627\\u06cc\\u062f \\u0631\\u0648\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u062a\\u0633\\u062a \\u0647\\u0645 \\u0627\\u0646\\u062c\\u0627\\u0645 \\u0628\\u062f\\u06cc\\u0645\\n\\n# \\u062e\\u0628 \\u062f\\u0627\\u062f\\u0647\\u0647\\u0627 \\u0646\\u0627\\u0644 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u0627\\u0648\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u06a9\\u0647 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0633\\u062a\\u06cc\\u0645 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0631\\u062f\\u06cc\\u0645 \\u0648 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u0648 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u0627\\u0648\\u0631\\u062f\\u06cc\\u0645 \\u062f\\u0627\\u062e\\u0644 \\u06cc\\u06a9 \\u0628\\u0627\\u0632\\u0647 \\u062e\\u0627\\u0635\\n# \\u06a9\\u0627\\u0631 \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc \\u062a\\u0645\\u0648\\u0645 \\u0634\\u062f\\n\\n\\n# ==============================Lable Encoder=============================================\\n\\n# \\u062d\\u0627\\u0644 \\u0646\\u0648\\u0628\\u062a \\u0627\\u06cc\\u0646\\u0647 \\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647\\u0627\\u06cc \\u0645\\u062a\\u0646\\u06cc \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645........\\n\\n# \\u062f\\u0648\\u062a\\u0627 \\u0631\\u0627 \\u0647 \\u062f\\u0627\\u0631\\u06cc\\u0645 \\u06cc\\u06a9\\u06cc\\u0634\\n# LableEncoder\\n# \\u0645\\u06cc\\u0627\\u062f \\u062f\\u0627\\u062f\\u0647\\u0627 \\u0631\\u0648 \\u0635\\u0641\\u0631 \\u0648 \\u06cc\\u06a9 \\u0645\\u06cc\\u06a9\\u0646\\u0647 ...\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n\\n# encoder=LabelEncoder()\\n# data_cat=df['ocean_proximity']\\n# data_cat_encoded=encoder.fit_transform(data_cat)\\n# data_cat_encoded=pd.DataFrame(data_cat_encoded,columns=['ocean_proximity'])\\n# data_cat_encoded.head()\\n\\n# \\u0627\\u06cc\\u0631\\u0627\\u062f \\u0627\\u06cc\\u0646 \\u0631\\u0648\\u0634\\n# \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0647\\u0627\\u06cc \\u0645\\u0627 \\u0631\\u0627\\u0628\\u0637\\u0647 \\u0628\\u06cc\\u0646 \\u0627\\u06cc\\u0646 \\u0627\\u0639\\u062f\\u0627\\u062f \\u062f\\u0631\\u0646\\u0638\\u0631 \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647\\n# \\u0645\\u062b\\u0644\\u0627 \\u0645\\u06cc\\u06af\\u0647 \\u0686\\u0648\\u0646 \\u0635\\u0641\\u0631 \\u0648\\u06cc\\u06a9 \\u0628\\u0647\\u0645 \\u0646\\u0632\\u062f\\u06cc\\u06a9 \\u0647\\u0633\\u062a\\u0646 \\u0628\\u0627\\u0647\\u0645 \\u0631\\u0627\\u0628\\u0637\\u0647 \\u062f\\u0627\\u0631\\u0646 \\u0627\\u0645\\u0627 \\u0628\\u0627 \\u06f3\\u0631\\u0627\\u0628\\u0637\\u0647 \\u0646\\u062f\\u0627\\u0631\\u0647\\n#\\n\\n# \\u061f \\u06a9\\u06cc \\u0627\\u0632 \\u0631\\u0648\\u0634 \\u06cc\\u06a9 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u06a9\\u0646\\u06cc\\u0645\\n# \\u0632\\u0645\\u0627\\u0646\\u06cc \\u06a9\\u0647 \\u0645\\u062b\\u0644\\u0627 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0645\\u0627 \\u0627\\u06cc\\u0646\\u062c\\u0648\\u0631\\u06cc \\u0628\\u0627\\u0634\\u0647 \\u062e\\u06cc\\u0644\\u06cc \\u062e\\u0648\\u0628 \\u060c \\u062e\\u0648\\u0628 \\u060c \\u0628\\u062f \\u060c \\u0645\\u062a\\u0648\\u0633\\u0637 \\u0648...\\n# \\u0627\\u06cc\\u0646 \\u062c\\u0648\\u0631\\u06cc \\u0628\\u0627\\u0634\\u0647\\n\\n\\n# \\u062f\\u0631 \\u0627\\u06cc\\u0646 \\u062d\\u0627\\u0644\\u062a \\u0627\\u0632 \\u0631\\u0648\\u0634 \\u062f\\u0648\\u0645 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645\\n# =============================================onehotEncoder ==================\\n# \\u0631\\u0648\\u0634 \\u062f\\u0648\\u0645 \\u0628\\u0631\\u0627\\u06cc \\u062a\\u0628\\u062f\\u06cc\\u0644 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0645\\u062a\\u0646\\u06cc \\u0628\\u0647 \\u0639\\u062f\\u062f\\u06cc\\n\\n# \\u0648\\u0642\\u062a\\u06cc \\u06a9\\u0647 \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0647\\u0627\\u06cc \\u06a9\\u0647 \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u062f \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u06a9\\u0646\\u0647 \\u06a9\\u0645 \\u0628\\u0627\\u0634\\u0647\\n\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n\\nencoder_1hot = OneHotEncoder(sparse=False)\\ndata_cat_1hot_tmp = encoder_1hot.fit_transform(df[[\\\"ocean_proximity\\\"]])\\n\\ndata_cat_1hot = pd.DataFrame(data_cat_1hot_tmp)\\ndata_cat_1hot.columns = encoder_1hot.get_feature_names([\\\"prox\\\"])\\n\\ndata_cat_1hot.head()\\n\\nfinal = pd.concat([data_num_scaled_tr, data_cat_1hot], axis=1)\\nfinal.head()\";\n",
       "                var nbb_formatted_code = \"from sklearn.base import BaseEstimator, TransformerMixin\\n\\n# ==prapare the data\\n\\n# numerical data            ===>         missing values\\n# categorical and text data ===>labelencoder ,onehotencoder\\n# numerical data            ===> feature scaling\\n# numerical data            ===> custom transformers\\n\\n# first make a anoter copy of train set\\ndf = train_set.copy()\\n# df.head()\\n# second you have find that witch data you dont wanna change and\\n# whtch data you wanna change\\n# \\u0686\\u0648\\u0646 \\u0645\\u0627 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0647\\u06cc\\u0645 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0642\\u06cc\\u0645\\u06cc\\u062a \\u062e\\u0648\\u0646\\u0647 \\u062f\\u0631 \\u06cc\\u06a9 \\u0645\\u0646\\u0637\\u0642\\u0647 \\u0631\\u0648 \\u067e\\u06cc\\u0634 \\u0628\\u06cc\\u0646\\u06cc \\u06a9\\u0646\\u06cc\\u0645 \\u0646\\u0628\\u0627\\u06cc\\u062f \\u0627\\u0648\\u0646 \\u062a\\u063a\\u06cc\\u06cc\\u0631 \\u06a9\\u0646\\u0647\\n# \\u067e\\u0633 \\u0645\\u06cc\\u0627\\u06cc\\u06cc\\u0645 \\u06cc\\u06a9 \\u06a9\\u067e\\u06cc \\u0627\\u0632 \\u0627\\u0648\\u0646 \\u0631\\u0648 \\u062a\\u0647\\u06cc\\u0647 \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645\\n# \\u0648\\u0627\\u06cc\\u0646\\u0648 \\u062c\\u062f\\u0627 \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645 \\u0627\\u0632 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u062f\\u06cc\\u06af\\u0647 \\u0648 \\u0628\\u0631\\u0627\\u06cc \\u067e\\u0627\\u06a9 \\u0633\\u0627\\u0632\\u06cc \\u0628\\u0647 \\u0627\\u06cc\\u0646 \\u0646\\u0628\\u0627\\u06cc\\u062f \\u062f\\u0633\\u062a \\u0628\\u0632\\u0646\\u06cc\\u0645...\\ndf_label = df[\\\"median_house_value\\\"].copy()\\ndf.drop(\\\"median_house_value\\\", axis=1)\\n# df.info()\\n\\n# \\u062e\\u0628 \\u0645\\u0627 \\u062f\\u0631 \\u0627\\u062a\\u0627\\u0642 \\u0647\\u0627\\u06cc \\u062e\\u0648\\u0627\\u0628 \\u0647\\u0627 \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0646\\u0627\\u0644 \\u062f\\u0627\\u0631\\u06cc\\u0645\\n# \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u0627\\u0648\\u0646 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645\\n# \\u0633\\u067e\\u0633 \\u0628\\u0627\\u06cc\\u062f \\u0645\\u0642\\u0627\\u062f\\u06cc\\u0631 \\u0645\\u062a\\u0646\\u06cc \\u0631\\u0648\\u0628\\u0627\\u06cc\\u062f \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645 \\u0686\\u0648\\u0646 \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0647\\u0627 \\u0627\\u06cc\\u0646\\u0627 \\u0631\\u0648 \\u0646\\u0645\\u06cc \\u0641\\u0647\\u0645\\u0646\\n\\n# df_num = df.drop(\\\"ocean_proximity\\\", axis=1)\\n# df_num.head()\\n# df_numd is all data that all of them are numercal ...\\n\\n# \\u062d\\u0627\\u0644 \\u0627\\u0632 \\u0628\\u06cc\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc \\u0645\\u0627 \\u0641\\u0642\\u0637 \\u062a\\u0639\\u0627\\u062f \\u0627\\u062a\\u0627\\u0642 \\u0647\\u0627 \\u0645\\u0634\\u06a9\\u0644 \\u062f\\u0627\\u0631\\u0647\\n# \\u062d\\u0627\\u0644 \\u0633\\u0648\\u0627\\u0644\\u06cc \\u06a9\\u0647 \\u067e\\u06cc\\u0633\\u0634 \\u0645\\u06cc\\u0627\\u062f \\u0627\\u06cc\\u0646\\u0647 \\u06a9\\u0647 \\u0627\\u0632 \\u06a9\\u062c\\u0627\\u0645\\u06cc\\u062f\\u0648\\u0646\\u06cc\\u0645 \\u0648\\u0642\\u062a\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u062c\\u062f\\u06cc\\u062f \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647 \\u0647\\u0645\\u06cc\\u0646\\n#\\n# \\u0633\\u062a\\u0648\\u0646 \\u0645\\u0634\\u06a9\\u0644 \\u062f\\u0627\\u0631\\u0647 \\u0648 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc \\u062f\\u06cc\\u06af\\u0647 \\u0645\\u0634\\u06a9\\u0644 \\u0646\\u062f\\u0627\\u0631\\u0647\\u061f\\n# \\u067e\\u0633 \\u0645\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u06cc\\u06a9 \\u0631\\u0627\\u0647\\u06a9\\u0627\\u0631 \\u06a9\\u0644\\u06cc \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645\\n\\n\\n# \\u0645\\u0627 \\u0633\\u0647 \\u062a\\u0627 \\u0627\\u067e\\u0634\\u0646 \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u062e\\u0631\\u0627\\u0628 \\u062f\\u0627\\u0631\\u06cc\\u0645\\n# \\u0627\\u067e\\u0634\\u0646 \\u06cc\\u06a9\\n# \\u0627\\u0648\\u0646 \\u0633\\u0637\\u0631 \\u0647\\u0627 \\u0631\\u0648 \\u0627\\u0632 \\u0628\\u06cc\\u0646 \\u0628\\u0628\\u0631\\u06cc\\u0645 \\u0633\\u0637\\u0631 \\u0647\\u0627\\u06cc \\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0646\\u0627\\u0644 \\u0631\\u0648 \\u0627\\u0632 \\u0628\\u06cc\\u0646  \\u0628\\u0628\\u0631\\u06cc\\u0645\\n\\n\\n# \\u0627\\u067e\\u0634\\u0646 \\u062f\\u0648\\u0645\\n\\n# \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646\\u06cc \\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0646\\u0627\\u0644 \\u062f\\u0627\\u0631\\u0647 \\u0631\\u0648 \\u0627\\u0632 \\u0628\\u06cc\\u0646 \\u0628\\u0628\\u0631\\u06cc\\u0645\\n# \\u0645\\u0645\\u06a9\\u0646\\u0647 \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0628\\u062f\\u0631\\u062f \\u0628\\u062e\\u0648\\u0631\\u0647 ...\\n\\n\\n# \\u0627\\u067e\\u0634\\u0646 \\u0633\\u0648\\u0645\\n# \\u0627\\u067e\\u0634\\u0646 \\u0633\\u0648\\u0645 \\u0627\\u06cc\\u0646\\u0647 \\u06a9\\u0647 \\u0628\\u06cc\\u0627\\u06cc\\u0645 \\u0645\\u0642\\u062f\\u0627\\u0631 \\u062c\\u0627\\u06cc\\u06af\\u0632\\u06cc\\u0646 \\u0628\\u062f\\u06cc\\u0645\\n\\n# ==================== simpleImputer\\n# \\u0622\\u067e\\u0634\\u0646 \\u06f1\\n# df_num=df_num.dropna(subset=['total_bedrooms']) #option 1\\n\\n\\n# df_num.drop('total_bedrooms',axis=1)     #option 2\\n\\n# optin 3\\n# use th max or min or meadin or a const number for total_bedrooms\\n\\n# median = df_num[\\\"total_bedrooms\\\"].median()\\n# df_num[\\\"total_bedrooms\\\"].fillna(median)\\n# df_num.info()\\n\\n# \\u062e\\u0628 \\u0645\\u0627 \\u0641\\u0647\\u0645\\u06cc\\u062f\\u06cc\\u0645 \\u0627\\u067e\\u0634\\u0646 \\u0633\\u0647 \\u0628\\u0647\\u062a\\u0631\\u0647\\n# \\u062d\\u0627\\u0644\\u0627 \\u0628\\u0631\\u0627\\u06cc \\u0628\\u06a9\\u0627\\u0631 \\u06af\\u06cc\\u0631\\u06cc \\u0627\\u0632 \\u0627\\u06cc\\u0646 \\u0631\\u0648\\u0634 \\u0627\\u0632 \\u062a\\u0627\\u0628\\u0639 \\u0632\\u06cc\\u0631 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc \\u06a9\\u0646\\u06cc\\u0645\\n\\n# \\u0648\\u0642\\u062a\\u06cc \\u0627\\u0632 \\u0627\\u06cc\\u0646 \\u062a\\u0627\\u0628\\u0639 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645 \\u062e\\u06cc\\u0627\\u0644\\u0645\\u0648\\u0646 \\u0627\\u0631\\u062d\\u062a\\u0647 \\u0647\\u0631 \\u0633\\u062a\\u0648\\u0646\\u06cc \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0646\\u0627\\u0644 \\u062f\\u0627\\u0634\\u062a \\u0628\\u0631\\u0627\\u0645\\u0648\\u0646\\u062f\\u0631\\u0633\\u062a\\u0634 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\n# medain \\u0645\\u06cc\\u0627\\u0646\\u0647\\n# mean \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646\\n# \\u0645\\u0645\\u06a9\\u0646\\u0647 \\u0645\\u0627 \\u06cc\\u0647 \\u0639\\u062f\\u062f \\u067e\\u0631\\u062a \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645 \\u062f\\u0627\\u062e\\u0644 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0645\\u0648\\u0646 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0645\\u06cc\\u0627\\u062f \\u0627\\u0648\\u0646 \\u0631\\u0648 \\u0647\\u0645 \\u062f\\u0631\\u0646\\u0638\\u0631\\n# \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647 \\u0627\\u0645\\u0627 \\u0645\\u06cc\\u0627\\u0646\\u0647 \\u0632\\u06cc\\u0627\\u062f \\u0646\\u0647 \\u0686\\u0648\\u0646\\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u0633\\u0631\\u062a \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n# \\u0628\\u0639\\u062f \\u062f\\u0627\\u062f\\u0647\\u0648\\u06cc\\u0637 \\u0631\\u0648 \\u0628\\u0631 \\u0645\\u06cc\\u062f\\u0627\\u0631\\u0647\\n\\n# \\u0645\\u0642\\u0627\\u062f\\u0631\\u06cc \\u06a9\\u0647 \\u0628\\u06cc\\u0634\\u062a\\u0631\\u06cc\\u0646 \\u062a\\u06a9\\u0631\\u0627\\u0631 \\u062f\\u0634\\u0627\\u062a\\u0647\\n# most frequence\\n# \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0645\\u062a\\u0646\\u06cc \\u0647\\u0645 \\u0628\\u06a9\\u0627\\u0631 \\u0645\\u06cc\\u0631\\u0647 ...\\n\\nimputer = SimpleImputer(missing_values=np.nan, strategy=\\\"median\\\")\\nimputer.fit(df_num)\\n# \\u062a\\u0627\\u0628\\u0639 \\u0641\\u06cc\\u062a \\u0647\\u0645\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647 \\u0648 \\u0627\\u0632 \\u0627\\u0648\\u0646 \\u06cc\\u0627\\u062f \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647\\nX = imputer.transform(df_num)\\n# \\u062a\\u0631\\u0646\\u0633\\u0641\\u0648\\u0631\\u0645 \\u0686\\u06cc\\u0632\\u06cc \\u06a9\\u0647 \\u062f\\u0631 \\u0641\\u06cc\\u062a \\u0628\\u062f\\u0633\\u062a \\u0627\\u0648\\u0631\\u062f\\u0647 \\u0631\\u0648 \\u062c\\u0627\\u06cc\\u06af\\u0632\\u06cc\\u0646 \\u0645\\u06cc \\u06a9\\u0646\\u0647\\n# \\u062d\\u0627\\u0635\\u0644 \\u062a\\u0627\\u0628\\u0639 \\u062a\\u0631\\u0646\\u0633\\u0641\\u0648\\u0631\\u0645 \\u06cc\\u06a9 \\u0627\\u0631\\u0627\\u06cc\\u0647 \\u0646\\u0627\\u0645 \\u067e\\u0627\\u06cc \\u0647\\u0633\\u062a\\n\\n# \\u0645\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u0628\\u0647 \\u062f\\u06cc\\u062a\\u0627 \\u0641\\u0631\\u0645 \\u062a\\u0628\\u062f\\u06cc\\u0644\\u0634 \\u06a9\\u0646\\u06cc\\u0645 ...\\n\\ndf_num_imput_er = pd.DataFrame(X, columns=df_num.columns)\\n# df_num_imput_er.info()\\n# df_num.info()\\n# df_num_imput_er.head()\\n\\n\\n# ========================================custom transformers\\n# \\u0645\\u0645\\u06a9\\u0646\\u0647 \\u0645\\u0627 \\u0686\\u0646\\u062f \\u06cc\\u0646  \\u0633\\u062a\\u0648\\u0646 \\u062c\\u062f\\u06cc\\u062f \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645 \\u06a9\\u0647 \\u0628\\u062f\\u0631\\u062f\\u0645\\u0648\\u0646 \\u0645\\u06cc\\u062e\\u0648\\u0631\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062a\\u0648\\u0646\\u06cc\\u0645\\n# \\u0627\\u06cc\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0631\\u0648 \\u0628\\u0647 \\u062f\\u0627\\u062f\\u0647\\u0647\\u0627 \\u0645\\u0648\\u0646 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0646\\u0647\\n\\n# \\u0645\\u062b\\u0644\\u0627 \\u0645\\u0627 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0647\\u06cc\\u0645 \\u06cc\\u06a9 \\u0633\\u0631\\u06cc \\u0633\\u062a\\u0648\\u0646 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0646\\u06cc\\u0645 \\u0627\\u0632 \\u0627\\u06cc\\u0646 \\u0631\\u0627\\u0647 \\u0645\\u06cc\\u0631\\u06cc\\u0645\\n# \\u0645\\u062b\\u0644\\u0627 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u06cc \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u06cc\\u06a9 \\u0633\\u062a\\u0648\\u0646 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u06a9\\u0646\\u06cc \\u0628\\u0631\\u06cc\\u0632\\u06cc \\u062a\\u0648\\u06cc \\u06cc\\u06a9 \\u0633\\u062a\\u0648\\u0646 \\u062f\\u06cc\\u06af\\u0647\\n\\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\\n\\n\\nclass CombineAtteributeAdder(BaseEstimator, TransformerMixin):\\n    #     \\u0686\\u0648\\u0646 \\u0645\\u0627 \\u06cc\\u0627\\u062f\\u06af\\u06cc\\u0631\\u06cc \\u0646\\u062f\\u0627\\u0631\\u06cc\\u0645 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062a\\u0627\\u0628\\u0639 \\u0641\\u06cc\\u062a \\u0646\\u06cc\\u0627\\u0632 \\u0646\\u06cc\\u0633\\u062a \\u06a9\\u0627\\u0631 \\u06a9\\u0646\\u0647 ...\\n    # \\u0627\\u0645\\u0627 \\u0645\\u062b\\u0644\\u0627 \\u0627\\u06af\\u0631 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0633\\u062a\\u06cc \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u06a9\\u0646\\u0647 \\u0628\\u0627\\u06cc\\u062f \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u06cc\\u06a9 \\u06a9\\u0627\\u0631\\u06cc \\u0627\\u0646\\u062c\\u0627\\u0645 \\u0645\\u06cc\\u062f\\u0627\\u062f\\u06cc\\n    #    \\u06cc\\u0639\\u0646\\u06cc \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0645\\u06cc\\u0646\\u06af\\u06cc\\u0646 \\u0631\\u0627 \\u0627\\u0628\\u062a\\u062f\\u0627 \\u062f\\u0631 \\u0641\\u06cc\\u062a \\u062d\\u0633\\u0627\\u0628 \\u0645\\u06cc\\u06a9\\u0646\\u06cc \\u0648 \\u0627\\u0632 \\u0627\\u0648\\u0646 \\u062f\\u0631 \\u062a\\u0631\\u0646\\u0633\\u0641\\u0648\\u0631\\u0645 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645\\n\\n    def fit(self, X, y=None):\\n        return self\\n\\n    def transform(self, X, y=None):\\n        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\\n        population_per_household = X[:, population_ix] / X[:, household_ix]\\n\\n        bedrooms_per_rooms = X[:, bedrooms_ix] / X[:, rooms_ix]\\n\\n        return np.c_[\\n            X, rooms_per_household, population_per_household, bedrooms_per_rooms\\n        ]\\n\\n\\ncustom = CombineAtteributeAdder()\\n# \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u0627\\u0632 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0627\\u06cc\\u067e\\u06cc\\u0648\\u0648\\u062a\\u0631 \\u0634\\u062f\\u0647 \\u06a9\\u0627 \\u0631 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645 \\u0686\\u0648\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0646\\u0627\\u0644 \\u0646\\u062f\\u0627\\u0631\\u06cc\\u0645\\ndata_sustom_tr_tmp = custom.transform(df_num_imput_er.values)\\n\\ndata_sustom_tr = pd.DataFrame(data_sustom_tr_tmp)\\n# \\u06cc\\u06a9 \\u0644\\u06cc\\u0633\\u062a \\u0645\\u06cc\\u0633\\u0627\\u0632\\u06cc\\u0645 \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0645\\u06cc\\u06af\\u06cc\\u0645 \\u0647\\u0645\\u0647 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc \\u0642\\u0628\\u0644\\u06cc \\u0631\\u0648 \\u062f\\u0627\\u0631\\u06cc\\u0645 \\u0628\\u0639\\u062f \\u0633\\u0647 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0645\\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u06a9\\u0647 \\u0628\\u0627 \\u0627\\u067e\\u0646\\u062f \\u0628\\u0647\\u0634 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645\\n\\ncolumns = list(df_num_imput_er.columns)\\ncolumns.append(\\\"rooms_per_household\\\")\\ncolumns.append(\\\"population_per_household\\\")\\ncolumns.append(\\\"bedrooms_per_rooms\\\")\\n\\ndata_sustom_tr.columns = columns\\ndata_sustom_tr.head(10)\\n# data_sustom_tr.shape\\n# data.shape\\n\\n# \\u067e\\u0633 \\u0645\\u0627 \\u062f\\u0631 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc  \\u062a\\u0627 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062f\\u0648\\u062a\\u0627 \\u06a9\\u0627\\u0631 \\u0627\\u0646\\u062c\\u0627\\u0645 \\u062f\\u0627\\u062f\\u06cc\\u0645\\n# \\u06cc\\u06a9\\u06cc \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0646\\u0627\\u0644 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u062f\\u0648\\u0645 \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0647\\u0627\\u06cc\\u06cc \\u06a9\\u0647 \\u067e\\u06cc\\u062f\\u0627 \\u06a9\\u0631\\u062f\\u0647 \\u0628\\u0648\\u062f\\u06cc\\u0645 \\u0631\\u0648 \\u0647\\u0645 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n\\n# \\u06cc\\u06a9 \\u0645\\u06cc\\u0634\\u0647 missing data\\n# \\u062f\\u0648 \\u0645\\u06cc\\u0634\\u0647 custom transform\\n# \\u06cc\\u06a9 \\u06a9\\u0627\\u0631 \\u062f\\u06cc\\u06af\\u0647 \\u0647\\u0645 \\u062f\\u0627\\u0631\\u06cc\\u0645 \\u06a9\\u0647 \\u0645\\u06cc\\u0634\\u0647\\n# feature scaling\\n# =============================================feature scaling===================================\\n#  \\u0645\\u0627 \\u0627\\u06af\\u0631 \\u06cc\\u06a9 \\u0633\\u0631\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645\\u0648 \\u0627\\u06cc\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u062e\\u06cc\\u0644\\u06cc \\u0645\\u062a\\u0646\\u0648\\u0639 \\u0628\\u0627\\u0634\\u0646\\n# \\u06cc\\u0639\\u0646\\u06cc \\u0628\\u06cc\\u0646 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0648 \\u0645\\u0627\\u06a9\\u0633\\u06cc\\u0645\\u0645 \\u0648 \\u0645\\u0646\\u06cc\\u0645\\u0645 \\u0627\\u0646\\u0647\\u0627 \\u0641\\u0627\\u0635\\u0644\\u0647 \\u0632\\u06cc\\u0627\\u062f\\u06cc \\u0628\\u0627\\u0634\\u0647\\n# \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0627\\u0631 \\u0646\\u0645\\u06cc\\u06a9\\u0646\\u0647\\n# \\u0628\\u0631\\u0627\\u06cc \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u0627\\u06cc\\u0646 \\u0645\\u0634\\u06a9\\u0644 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645\\n\\n# \\u0627\\u0628\\u062a\\u062f\\u0627 \\u062a\\u0627\\u0628\\u0639\\n# describe()\\n# \\u0631\\u0648 \\u0635\\u062f\\u0627 \\u0645\\u06cc\\u0632\\u0646\\u06cc\\u0645.\\n\\ndata_sustom_tr.describe()\\n\\n\\n# \\u0648\\u0642\\u062a\\u06cc \\u0627\\u06cc\\u0646\\u0648 \\u0635\\u062f\\u0627 \\u0628\\u0632\\u0646\\u06cc \\u0645\\u06cc\\u0628\\u06cc\\u0646\\u06cc \\u06a9\\u0647\\u0645\\u062b\\u0644\\u0627 \\u062f\\u0631\\n# total_bedrooms\\n# \\u0645\\u0646\\u06cc\\u0645\\u0645\\u0634 \\u06f2 \\u0647\\u0633\\u062a \\u0648 \\u0645\\u0627\\u06a9\\u0633\\u06cc\\u0645\\u0645\\u0634\\n# \\u06f6\\u06f0\\u06f0\\u06f0\\n# \\u062e\\u06cc\\u0644\\u06cc\\u06cc\\u06cc \\u0641\\u0627\\u0635\\u0644\\u0647 \\u0627\\u0633\\u062a\\u062a.\\n# \\u0628\\u0627\\u06cc\\u062f \\u0627\\u06cc\\u0646 \\u0641\\u0627\\u0635\\u0644\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645\\n\\n\\n# \\u062f\\u0648 \\u0631\\u0648\\u0634 \\u0628\\u0631\\u0627\\u06cc \\u0627\\u06cc\\u0646\\u06a9\\u0647 \\u0627\\u06cc\\u0646\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645 \\u0648\\u062c\\u0648\\u062f \\u062f\\u0627\\u0631\\u0647\\n\\n# 1-standardization\\n\\n\\n# 2-noramlizatin [0,1]\\n\\n# \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u062f\\u0631 \\u0627\\u06cc\\u0646 \\u0628\\u0627\\u0632\\u0647 \\u0645\\u06cc\\u0627\\u0631\\u0634\\u0648\\u0646\\n# \\u0648 \\u062a\\u063a\\u06cc\\u06cc\\u0631 \\u0645\\u0642\\u06cc\\u0627\\u0633 \\u0645\\u06cc\\u062f\\u0647 ...\\n\\n# \\u0645\\u0646\\u0647\\u0627\\u06cc \\u0645\\u0646\\u06cc\\u0645\\u0645 \\u0645\\u06cc\\u06a9\\u0646\\u0647 \\u0627\\u0628\\u062a\\u062f\\u0627 \\u0628\\u0639\\u062f\\u0634 \\u062a\\u0642\\u0633\\u06cc\\u0645 \\u0628\\u0631 \\u0645\\u0627\\u06a9\\u0633 \\u0645\\u0646\\u0647\\u0627\\u06cc \\u0645\\u06cc\\u0646 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\n# \\u06cc\\u06a9 \\u0627\\u06cc\\u0631\\u0627\\u062f \\u0628\\u0632\\u0631\\u06af \\u0648\\u062c\\u0648\\u062f \\u062f\\u0627\\u0631\\u0647 \\u0627\\u06cc\\u0631\\u0627\\u062f\\u0634 \\u0632\\u0645\\u0627\\u0646\\u06cc\\u0647 \\u06a9\\u0647 \\u06cc\\u06a9 \\u062f\\u0627\\u062f\\u0647 \\u067e\\u0631\\u062a \\u062f\\u0627\\u0634\\u062a\\u0647 \\u0628\\u0627\\u0634\\u06cc\\u0645\\n# \\u0645\\u062b\\u0644\\u0627 \\u0627\\u06af\\u0631 \\u0645\\u0642\\u0627\\u062f\\u06cc\\u0631 \\u0645\\u0627 \\u0628\\u0647 \\u0635\\u0648\\u0631\\u062a \\u0632\\u06cc\\u0631 \\u0628\\u0627\\u0634\\u062f\\n# 1,3,4,5,7,100\\n\\n# \\u0627\\u06cc\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u06f1\\u06f0\\u06f0 \\u062f\\u0627\\u062f\\u0647 \\u067e\\u0631\\u062a\\u0647\\n\\n# \\u06a9\\u0644 \\u0627\\u0639\\u062f\\u0627\\u062f \\u0631\\u0648 \\u0628\\u06cc\\u0646 \\u0635\\u0641\\u0631 \\u062a\\u0627 \\u0647\\u0634\\u062a \\u062f\\u0647\\u0645 \\u0642\\u0631\\u0627\\u0631 \\u0645\\u06cc\\u062f\\u0647 \\u0648 \\u06f1\\u06f0\\u06f0 \\u0631\\u0648 \\u06cc\\u06a9 \\u0642\\u0631\\u0627\\u0631 \\u0645\\u06cc\\u062f\\u0647...\\n\\n\\n# \\u0645\\u0632\\u06cc\\u062a \\u0646\\u0631\\u0645\\u0627\\u0644\\u06cc\\u0632\\u0634\\u0646\\n#\\n# \\u062f\\u0631 \\u0634\\u0628\\u06a9\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u0635\\u0628\\u06cc \\u062e\\u0648\\u0628\\u0647 \\u0686\\u0648\\u0646 \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0647\\u0627\\u06cc \\u0634\\u0628\\u06a9\\u0647 \\u0647\\u0627\\u06cc \\u0639\\u0635\\u0628\\u06cc \\u062f\\u0631 \\u0635\\u0641\\u0631 \\u0648 \\u06cc\\u06a9 \\u06a9\\u0627\\u0631 \\u0645\\u06cc\\u06a9\\u0646\\u0646\\n\\n\\n# 1-standardization\\n# \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u0645\\u06cc\\u0627\\u062f \\u0648\\u0627\\u0631\\u06cc\\u0627\\u0646\\u0633 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\n# \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0631\\u0627 \\u0645\\u0646\\u0647\\u0627\\u06cc \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0645\\u06cc\\u0627\\u0646\\u06af\\u06cc\\u0646 \\u0645\\u06cc\\u06a9\\u0646\\u0647 \\u0648 \\u0633\\u067e\\u0633 \\u062a\\u0642\\u0633\\u06cc\\u0645 \\u0628\\u0631 \\u0648\\u0627\\u0631\\u06cc\\u0627\\u0646\\u0633 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n#\\n# \\u06cc\\u06a9 \\u0628\\u0627\\u0632\\u0647 \\u062c\\u062f\\u06cc\\u062f \\u0628\\u0647 \\u0645\\u0627 \\u0645\\u06cc\\u062f\\u0647 \\u0648 \\u0641\\u0627\\u0635\\u0644\\u0647\\u0647\\u0627 \\u0631\\u0648 \\u062d\\u0633\\u0627\\u0628 \\u0645\\u06cc\\u06a9\\u0646\\u0647\\n\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\n# \\u0645\\u0627 \\u0628\\u0627 \\u0627\\u06cc\\u0646 \\u062a\\u0627\\u0628\\u0639 \\u062a\\u0645\\u0627\\u0645 \\u0645\\u0642\\u0627\\u062f\\u06cc\\u0631 \\u062f\\u0627\\u062f\\u0647 \\u0627\\u06cc \\u0631\\u0648 \\u0645\\u06cc\\u0627\\u0631\\u06cc\\u0645 \\u0631\\u0648\\u06cc \\u06cc\\u06a9 \\u0628\\u0627\\u0632\\u0647 \\u062e\\u0627\\u0635 \\u06a9\\u0647 \\u0628\\u0631\\u0627\\u06cc \\u0627\\u06cc\\u0646 \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0645\\u0627 \\u0645\\u0646\\u0627\\u0633\\u0628\\u0647\\n\\nfeature_scal = StandardScaler()\\ndata_num_scaled_tr = pd.DataFrame(\\n    feature_scal.fit_transform(data_sustom_tr.values), columns=data_sustom_tr.columns\\n)\\n\\n\\ndata_num_scaled_tr.head()\\n\\n\\n#\\n# \\u062a\\u063a\\u06cc\\u06cc\\u0631\\u0627\\u062a\\u06cc \\u06a9\\u0647 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062f\\u0627\\u062f\\u06cc\\u0645 \\u0628\\u0627\\u06cc\\u062f \\u0631\\u0648\\u06cc \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u062a\\u0633\\u062a \\u0647\\u0645 \\u0627\\u0646\\u062c\\u0627\\u0645 \\u0628\\u062f\\u06cc\\u0645\\n\\n# \\u062e\\u0628 \\u062f\\u0627\\u062f\\u0647\\u0647\\u0627 \\u0646\\u0627\\u0644 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u0627\\u0648\\u0646 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u06a9\\u0647 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u0633\\u062a\\u06cc\\u0645 \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0631\\u062f\\u06cc\\u0645 \\u0648 \\u0627\\u0636\\u0627\\u0641\\u0647 \\u06a9\\u0631\\u062f\\u06cc\\u0645\\n# \\u0648 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627 \\u0631\\u0648 \\u0627\\u0648\\u0631\\u062f\\u06cc\\u0645 \\u062f\\u0627\\u062e\\u0644 \\u06cc\\u06a9 \\u0628\\u0627\\u0632\\u0647 \\u062e\\u0627\\u0635\\n# \\u06a9\\u0627\\u0631 \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0627\\u062f \\u0647\\u0627\\u06cc \\u0639\\u062f\\u062f\\u06cc \\u062a\\u0645\\u0648\\u0645 \\u0634\\u062f\\n\\n\\n# ==============================Lable Encoder=============================================\\n\\n# \\u062d\\u0627\\u0644 \\u0646\\u0648\\u0628\\u062a \\u0627\\u06cc\\u0646\\u0647 \\u06a9\\u0647 \\u062f\\u0627\\u062f\\u0647\\u0627\\u06cc \\u0645\\u062a\\u0646\\u06cc \\u0631\\u0648 \\u062f\\u0631\\u0633\\u062a \\u06a9\\u0646\\u06cc\\u0645........\\n\\n# \\u062f\\u0648\\u062a\\u0627 \\u0631\\u0627 \\u0647 \\u062f\\u0627\\u0631\\u06cc\\u0645 \\u06cc\\u06a9\\u06cc\\u0634\\n# LableEncoder\\n# \\u0645\\u06cc\\u0627\\u062f \\u062f\\u0627\\u062f\\u0647\\u0627 \\u0631\\u0648 \\u0635\\u0641\\u0631 \\u0648 \\u06cc\\u06a9 \\u0645\\u06cc\\u06a9\\u0646\\u0647 ...\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n\\n# encoder=LabelEncoder()\\n# data_cat=df['ocean_proximity']\\n# data_cat_encoded=encoder.fit_transform(data_cat)\\n# data_cat_encoded=pd.DataFrame(data_cat_encoded,columns=['ocean_proximity'])\\n# data_cat_encoded.head()\\n\\n# \\u0627\\u06cc\\u0631\\u0627\\u062f \\u0627\\u06cc\\u0646 \\u0631\\u0648\\u0634\\n# \\u0627\\u0644\\u06af\\u0648\\u0631\\u06cc\\u062a\\u0645 \\u0647\\u0627\\u06cc \\u0645\\u0627 \\u0631\\u0627\\u0628\\u0637\\u0647 \\u0628\\u06cc\\u0646 \\u0627\\u06cc\\u0646 \\u0627\\u0639\\u062f\\u0627\\u062f \\u062f\\u0631\\u0646\\u0638\\u0631 \\u0645\\u06cc\\u06af\\u06cc\\u0631\\u0647\\n# \\u0645\\u062b\\u0644\\u0627 \\u0645\\u06cc\\u06af\\u0647 \\u0686\\u0648\\u0646 \\u0635\\u0641\\u0631 \\u0648\\u06cc\\u06a9 \\u0628\\u0647\\u0645 \\u0646\\u0632\\u062f\\u06cc\\u06a9 \\u0647\\u0633\\u062a\\u0646 \\u0628\\u0627\\u0647\\u0645 \\u0631\\u0627\\u0628\\u0637\\u0647 \\u062f\\u0627\\u0631\\u0646 \\u0627\\u0645\\u0627 \\u0628\\u0627 \\u06f3\\u0631\\u0627\\u0628\\u0637\\u0647 \\u0646\\u062f\\u0627\\u0631\\u0647\\n#\\n\\n# \\u061f \\u06a9\\u06cc \\u0627\\u0632 \\u0631\\u0648\\u0634 \\u06cc\\u06a9 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u06a9\\u0646\\u06cc\\u0645\\n# \\u0632\\u0645\\u0627\\u0646\\u06cc \\u06a9\\u0647 \\u0645\\u062b\\u0644\\u0627 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0645\\u0627 \\u0627\\u06cc\\u0646\\u062c\\u0648\\u0631\\u06cc \\u0628\\u0627\\u0634\\u0647 \\u062e\\u06cc\\u0644\\u06cc \\u062e\\u0648\\u0628 \\u060c \\u062e\\u0648\\u0628 \\u060c \\u0628\\u062f \\u060c \\u0645\\u062a\\u0648\\u0633\\u0637 \\u0648...\\n# \\u0627\\u06cc\\u0646 \\u062c\\u0648\\u0631\\u06cc \\u0628\\u0627\\u0634\\u0647\\n\\n\\n# \\u062f\\u0631 \\u0627\\u06cc\\u0646 \\u062d\\u0627\\u0644\\u062a \\u0627\\u0632 \\u0631\\u0648\\u0634 \\u062f\\u0648\\u0645 \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u0645\\u06cc\\u06a9\\u0646\\u06cc\\u0645\\n# =============================================onehotEncoder ==================\\n# \\u0631\\u0648\\u0634 \\u062f\\u0648\\u0645 \\u0628\\u0631\\u0627\\u06cc \\u062a\\u0628\\u062f\\u06cc\\u0644 \\u062f\\u0627\\u062f\\u0647 \\u0647\\u0627\\u06cc \\u0645\\u062a\\u0646\\u06cc \\u0628\\u0647 \\u0639\\u062f\\u062f\\u06cc\\n\\n# \\u0648\\u0642\\u062a\\u06cc \\u06a9\\u0647 \\u0645\\u0642\\u062f\\u0627\\u0631 \\u0647\\u0627\\u06cc \\u06a9\\u0647 \\u0627\\u0648\\u0646 \\u0633\\u062a\\u0648\\u0646 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u062f \\u0627\\u0633\\u062a\\u0641\\u0627\\u062f\\u0647 \\u06a9\\u0646\\u0647 \\u06a9\\u0645 \\u0628\\u0627\\u0634\\u0647\\n\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n\\nencoder_1hot = OneHotEncoder(sparse=False)\\ndata_cat_1hot_tmp = encoder_1hot.fit_transform(df[[\\\"ocean_proximity\\\"]])\\n\\ndata_cat_1hot = pd.DataFrame(data_cat_1hot_tmp)\\ndata_cat_1hot.columns = encoder_1hot.get_feature_names([\\\"prox\\\"])\\n\\ndata_cat_1hot.head()\\n\\nfinal = pd.concat([data_num_scaled_tr, data_cat_1hot], axis=1)\\nfinal.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# ==prapare the data\n",
    "\n",
    "# numerical data            ===>         missing values\n",
    "# categorical and text data ===>labelencoder ,onehotencoder\n",
    "# numerical data            ===> feature scaling\n",
    "# numerical data            ===> custom transformers\n",
    "\n",
    "# first make a anoter copy of train set\n",
    "df = train_set.copy()\n",
    "# df.head()\n",
    "# second you have find that witch data you dont wanna change and\n",
    "# whtch data you wanna change\n",
    "# چون ما میخواهیم میانگین قیمیت خونه در یک منطقه رو پیش بینی کنیم نباید اون تغییر کنه\n",
    "# پس میاییم یک کپی از اون رو تهیه می کنیم\n",
    "# واینو جدا می کنیم از داده ها دیگه و برای پاک سازی به این نباید دست بزنیم...\n",
    "df_label = df[\"median_house_value\"].copy()\n",
    "df.drop(\"median_house_value\", axis=1)\n",
    "# df.info()\n",
    "\n",
    "# خب ما در اتاق های خواب ها مقدار نال داریم\n",
    "# ابتدا باید اون رو درست کنیم\n",
    "# سپس باید مقادیر متنی روباید درست کنیم چون الگوریتم ها اینا رو نمی فهمن\n",
    "\n",
    "# df_num = df.drop(\"ocean_proximity\", axis=1)\n",
    "# df_num.head()\n",
    "# df_numd is all data that all of them are numercal ...\n",
    "\n",
    "# حال از بین داده های عددی ما فقط تعاد اتاق ها مشکل داره\n",
    "# حال سوالی که پیسش میاد اینه که از کجامیدونیم وقتی داده جدید به ما میده همین\n",
    "#\n",
    "# ستون مشکل داره و ستون های دیگه مشکل نداره؟\n",
    "# پس ما باید یک راهکار کلی برای داده های عددی داشته باشیم\n",
    "\n",
    "\n",
    "# ما سه تا اپشن برای داده ها خراب داریم\n",
    "# اپشن یک\n",
    "# اون سطر ها رو از بین ببریم سطر های که داده نال رو از بین  ببریم\n",
    "\n",
    "\n",
    "# اپشن دوم\n",
    "\n",
    "# اون ستونی که داده نال داره رو از بین ببریم\n",
    "# ممکنه اون ستون بدرد بخوره ...\n",
    "\n",
    "\n",
    "# اپشن سوم\n",
    "# اپشن سوم اینه که بیایم مقدار جایگزین بدیم\n",
    "\n",
    "# ==================== simpleImputer\n",
    "# آپشن ۱\n",
    "# df_num=df_num.dropna(subset=['total_bedrooms']) #option 1\n",
    "\n",
    "\n",
    "# df_num.drop('total_bedrooms',axis=1)     #option 2\n",
    "\n",
    "# optin 3\n",
    "# use th max or min or meadin or a const number for total_bedrooms\n",
    "\n",
    "# median = df_num[\"total_bedrooms\"].median()\n",
    "# df_num[\"total_bedrooms\"].fillna(median)\n",
    "# df_num.info()\n",
    "\n",
    "# خب ما فهمیدیم اپشن سه بهتره\n",
    "# حالا برای بکار گیری از این روش از تابع زیر استفاده می کنیم\n",
    "\n",
    "# وقتی از این تابع استفاده میکنیم خیالمون ارحته هر ستونی مقدار نال داشت براموندرستش میکنه\n",
    "\n",
    "# medain میانه\n",
    "# mean میانگین\n",
    "# ممکنه ما یه عدد پرت داشته باشیم داخل داده ها مون میانگین میاد اون رو هم درنظر\n",
    "# میگیره اما میانه زیاد نه چونداده ها رو سرت میکنه\n",
    "# بعد دادهویط رو بر میداره\n",
    "\n",
    "# مقادری که بیشترین تکرار دشاته\n",
    "# most frequence\n",
    "# برای داده ها متنی هم بکار میره ...\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "imputer.fit(df_num)\n",
    "# تابع فیت همه داده ها رو میگیره و از اون یاد میگیره\n",
    "X = imputer.transform(df_num)\n",
    "# ترنسفورم چیزی که در فیت بدست اورده رو جایگزین می کنه\n",
    "# حاصل تابع ترنسفورم یک ارایه نام پای هست\n",
    "\n",
    "# ما باید به دیتا فرم تبدیلش کنیم ...\n",
    "\n",
    "df_num_imput_er = pd.DataFrame(X, columns=df_num.columns)\n",
    "# df_num_imput_er.info()\n",
    "# df_num.info()\n",
    "# df_num_imput_er.head()\n",
    "\n",
    "\n",
    "# ========================================custom transformers\n",
    "# ممکنه ما چند ین  ستون جدید پیدا کرده باشیم که بدردمون میخوره ما میتونیم\n",
    "# این ستون رو به دادهها مون اضافه کنه\n",
    "\n",
    "# مثلا ما میخواهیم یک سری ستون اضافه کنیم از این راه میریم\n",
    "# مثلا میخوای میانگین یک ستون رو حساب کنی بریزی توی یک ستون دیگه\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "\n",
    "class CombineAtteributeAdder(BaseEstimator, TransformerMixin):\n",
    "    #     چون ما یادگیری نداریم اینجا تابع فیت نیاز نیست کار کنه ...\n",
    "    # اما مثلا اگر میخواستی میانگین رو حساب کنه باید اینجا یک کاری انجام میدادی\n",
    "    #    یعنی ابتدا مینگین را ابتدا در فیت حساب میکنی و از اون در ترنسفورم استفاده میکنیم\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "\n",
    "        bedrooms_per_rooms = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "\n",
    "        return np.c_[\n",
    "            X, rooms_per_household, population_per_household, bedrooms_per_rooms\n",
    "        ]\n",
    "\n",
    "\n",
    "custom = CombineAtteributeAdder()\n",
    "# اینجا از داده های ایپیووتر شده کا ر میکنیم چون داده نال نداریم\n",
    "data_sustom_tr_tmp = custom.transform(df_num_imput_er.values)\n",
    "\n",
    "data_sustom_tr = pd.DataFrame(data_sustom_tr_tmp)\n",
    "# یک لیست میسازیم ابتدا میگیم همه ستون های قبلی رو داریم بعد سه ستون هماضافه کردیم\n",
    "# که با اپند بهش اضافه میکنیم\n",
    "\n",
    "columns = list(df_num_imput_er.columns)\n",
    "columns.append(\"rooms_per_household\")\n",
    "columns.append(\"population_per_household\")\n",
    "columns.append(\"bedrooms_per_rooms\")\n",
    "\n",
    "data_sustom_tr.columns = columns\n",
    "data_sustom_tr.head(10)\n",
    "# data_sustom_tr.shape\n",
    "# data.shape\n",
    "\n",
    "# پس ما در داده های عددی  تا اینجا دوتا کار انجام دادیم\n",
    "# یکی اینکه داده های نال رو درست کردیم\n",
    "# دوم اینکه اون ستون هایی که پیدا کرده بودیم رو هم اضافه کردیم\n",
    "\n",
    "# یک میشه missing data\n",
    "# دو میشه custom transform\n",
    "# یک کار دیگه هم داریم که میشه\n",
    "# feature scaling\n",
    "# =============================================feature scaling===================================\n",
    "#  ما اگر یک سری داده داشته باشیمو این داده ها خیلی متنوع باشن\n",
    "# یعنی بین میانگین و ماکسیمم و منیمم انها فاصله زیادی باشه\n",
    "# الگوریتم درست کار نمیکنه\n",
    "# برای اینکه این مشکل رو درست کنیم\n",
    "\n",
    "# ابتدا تابع\n",
    "# describe()\n",
    "# رو صدا میزنیم.\n",
    "\n",
    "data_sustom_tr.describe()\n",
    "\n",
    "\n",
    "# وقتی اینو صدا بزنی میبینی کهمثلا در\n",
    "# total_bedrooms\n",
    "# منیممش ۲ هست و ماکسیممش\n",
    "# ۶۰۰۰\n",
    "# خیلییی فاصله استت.\n",
    "# باید این فاصله ها رو درست کنیم\n",
    "\n",
    "\n",
    "# دو روش برای اینکه اینو درست کنیم وجود داره\n",
    "\n",
    "# 1-standardization\n",
    "\n",
    "\n",
    "# 2-noramlizatin [0,1]\n",
    "\n",
    "# داده ها رو در این بازه میارشون\n",
    "# و تغییر مقیاس میده ...\n",
    "\n",
    "# منهای منیمم میکنه ابتدا بعدش تقسیم بر ماکس منهای مین میکنه\n",
    "\n",
    "# یک ایراد بزرگ وجود داره ایرادش زمانیه که یک داده پرت داشته باشیم\n",
    "# مثلا اگر مقادیر ما به صورت زیر باشد\n",
    "# 1,3,4,5,7,100\n",
    "\n",
    "# این داده ۱۰۰ داده پرته\n",
    "\n",
    "# کل اعداد رو بین صفر تا هشت دهم قرار میده و ۱۰۰ رو یک قرار میده...\n",
    "\n",
    "\n",
    "# مزیت نرمالیزشن\n",
    "#\n",
    "# در شبکه های عصبی خوبه چون الگوریتم های شبکه های عصبی در صفر و یک کار میکنن\n",
    "\n",
    "\n",
    "# 1-standardization\n",
    "# اینجا میاد واریانس رو حساب میکنه\n",
    "\n",
    "# مقدار را منهای مقدار میانگین میکنه و سپس تقسیم بر واریانس میکنه\n",
    "#\n",
    "# یک بازه جدید به ما میده و فاصلهها رو حساب میکنه\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# ما با این تابع تمام مقادیر داده ای رو میاریم روی یک بازه خاص که برای این الگوریتم ما مناسبه\n",
    "\n",
    "feature_scal = StandardScaler()\n",
    "data_num_scaled_tr = pd.DataFrame(\n",
    "    feature_scal.fit_transform(data_sustom_tr.values), columns=data_sustom_tr.columns\n",
    ")\n",
    "\n",
    "\n",
    "data_num_scaled_tr.head()\n",
    "\n",
    "\n",
    "#\n",
    "# تغییراتی که اینجا دادیم باید روی داده های تست هم انجام بدیم\n",
    "\n",
    "# خب دادهها نال رو درست کردیم\n",
    "# اون داده های که میخواستیم رو درست کردیم و اضافه کردیم\n",
    "# و داده ها رو اوردیم داخل یک بازه خاص\n",
    "# کار برای داد های عددی تموم شد\n",
    "\n",
    "\n",
    "# ==============================Lable Encoder=============================================\n",
    "\n",
    "# حال نوبت اینه که دادهای متنی رو درست کنیم........\n",
    "\n",
    "# دوتا را ه داریم یکیش\n",
    "# LableEncoder\n",
    "# میاد دادها رو صفر و یک میکنه ...\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# encoder=LabelEncoder()\n",
    "# data_cat=df['ocean_proximity']\n",
    "# data_cat_encoded=encoder.fit_transform(data_cat)\n",
    "# data_cat_encoded=pd.DataFrame(data_cat_encoded,columns=['ocean_proximity'])\n",
    "# data_cat_encoded.head()\n",
    "\n",
    "# ایراد این روش\n",
    "# الگوریتم های ما رابطه بین این اعداد درنظر میگیره\n",
    "# مثلا میگه چون صفر ویک بهم نزدیک هستن باهم رابطه دارن اما با ۳رابطه نداره\n",
    "#\n",
    "\n",
    "# ؟ کی از روش یک استفاده کنیم\n",
    "# زمانی که مثلا داده های ما اینجوری باشه خیلی خوب ، خوب ، بد ، متوسط و...\n",
    "# این جوری باشه\n",
    "\n",
    "\n",
    "# در این حالت از روش دوم استفاده میکنیم\n",
    "# =============================================onehotEncoder ==================\n",
    "# روش دوم برای تبدیل داده های متنی به عددی\n",
    "\n",
    "# وقتی که مقدار های که اون ستون میخواد استفاده کنه کم باشه\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "encoder_1hot = OneHotEncoder(sparse=False)\n",
    "data_cat_1hot_tmp = encoder_1hot.fit_transform(df[[\"ocean_proximity\"]])\n",
    "\n",
    "data_cat_1hot = pd.DataFrame(data_cat_1hot_tmp)\n",
    "data_cat_1hot.columns = encoder_1hot.get_feature_names([\"prox\"])\n",
    "\n",
    "data_cat_1hot.head()\n",
    "\n",
    "\n",
    "# حالا داده های عددی و متنی که جدا کردیم ودرستشون کردیم رو به هم می چسبونیم...\n",
    "\n",
    "final = pd.concat([data_num_scaled_tr, data_cat_1hot], axis=1)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128aeed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
